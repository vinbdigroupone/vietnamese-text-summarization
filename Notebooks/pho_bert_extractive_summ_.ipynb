{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pho_bert_extractive_summ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thKDQ3HZlxXB",
        "outputId": "63256c1e-ca91-40ad-dbd4-40b666f048b9"
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers\n",
        "!pip3 install fairseq\n",
        "!pip3 install fastBPE\n",
        "!pip install underthesea"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/af/f7a74e19d73ae492fc3171417b4d348fbdff90745867be1f048cd01d7c59/boto3-1.16.22-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Collecting botocore<1.20.0,>=1.19.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/7e/4ac4d23d11e5e0b451cb5fb81e13f11ef5965d27ff9936884c2a6cc91c48/botocore-1.19.22-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 21.9MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.22->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.22->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.22 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.22 botocore-1.19.22 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n",
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/5f/fddba88a1478e6223241065f779e3eb547e0a4db0a16ae46a2cf92a257b9/fairseq-0.10.0.tar.gz (677kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 5.1MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/1f/7f502b9e37596164111655861370b08626f46f9e4524433c354f472765d4/hydra_core-1.0.4-py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 24.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.5.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (3.3.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 30.3MB/s \n",
            "\u001b[?25hCollecting omegaconf>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 33.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.10.0-cp36-cp36m-linux_x86_64.whl size=2631901 sha256=af21812f7159b08a5af5e82920b566998bbeabaf6d35181fe109bbf840ddd299\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/23/f1/b5dba0d2da81577b3c426ef983e4bc021e5f751f35c5ba94a2\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime, PyYAML\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=f946e9fb5e0a5756a0780c52722796b9f2f44560bb6510e14223b86c775ed764\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=9a507aa23748d7b47cb81333ba3d1f3fc51b5396e774a57a0fdefa86def11191\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built antlr4-python3-runtime PyYAML\n",
            "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, portalocker, sacrebleu, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 fairseq-0.10.0 hydra-core-1.0.4 omegaconf-2.0.5 portalocker-2.0.0 sacrebleu-1.4.14\n",
            "Collecting fastbpe\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Building wheels for collected packages: fastbpe\n",
            "  Building wheel for fastbpe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastbpe: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481492 sha256=c589ac64dccb385b10fc3dc75d6ca674097352a8b593d7d34b74e75a4ace01bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "Successfully built fastbpe\n",
            "Installing collected packages: fastbpe\n",
            "Successfully installed fastbpe-0.1.0\n",
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.6.20)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645934 sha256=86c23d2b4af2a8fdab52b4f0d756fcf8833e979435940875dba10d89ff59eb19\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 29.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=182bd9b942b024558f427c0dc42b7c8ab14bf69db275ff661fad1157515a9f46\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.5.3)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.14)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.0.4)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.7)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (4.8)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (3.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (2.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.6/dist-packages (from omegaconf>=2.0.5->hydra-core->fairseq) (5.3.1)\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/22/1f5a4e4ae4dc318455cbbf2a154d557b28cc56437adce75583d2e22c38c3/underthesea-1.2.2-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from underthesea) (4.41.1)\n",
            "Collecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.8.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.17.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from underthesea) (7.1.2)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 52.6MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2020.6.20)\n",
            "Installing collected packages: scikit-learn, unidecode, python-crfsuite, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 underthesea-1.2.2 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMLDyB-2mISX",
        "outputId": "f7c21b04-6755-46ad-f6a4-d8a23e0bd3ce"
      },
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-20 17:09:00--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 13.225.103.51, 13.225.103.117, 13.225.103.113, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|13.225.103.51|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G  91.0MB/s    in 13s     \n",
            "\n",
            "2020-11-20 17:09:15 (90.1 MB/s) - ‘PhoBERT_base_fairseq.tar.gz’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ALpABKCmMKb",
        "outputId": "2945101f-7114-43ac-8b42-07a695cecaf2"
      },
      "source": [
        "!ls PhoBERT_base_fairseq"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes  dict.txt  model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91kZob2ql49L",
        "outputId": "a3b25a5e-1e5d-4da1-aff7-31b5a4fb3f00"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFGavbUcmBkp"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import torch \n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK2_V3_hm6OP",
        "outputId": "91c20d60-f2eb-4fff-d32a-9bdc17a436ef"
      },
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-20 17:09:46 | INFO | fairseq.file_utils | loading archive file PhoBERT_base_fairseq\n",
            "2020-11-20 17:09:47 | INFO | fairseq.tasks.masked_lm | dictionary: 64000 types\n",
            "2020-11-20 17:09:53 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json not found in cache, downloading to /tmp/tmpoi658fxi\n",
            "1042301B [00:00, 1243835.49B/s]\n",
            "2020-11-20 17:09:55 | INFO | fairseq.file_utils | copying /tmp/tmpoi658fxi to cache at /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2020-11-20 17:09:55 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2020-11-20 17:09:55 | INFO | fairseq.file_utils | removing temp file /tmp/tmpoi658fxi\n",
            "2020-11-20 17:09:55 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe not found in cache, downloading to /tmp/tmpvoqj7bw4\n",
            "456318B [00:00, 718599.43B/s]\n",
            "2020-11-20 17:09:57 | INFO | fairseq.file_utils | copying /tmp/tmpvoqj7bw4 to cache at /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2020-11-20 17:09:57 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2020-11-20 17:09:57 | INFO | fairseq.file_utils | removing temp file /tmp/tmpvoqj7bw4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerSentenceEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layers): ModuleList(\n",
              "          (0): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): TransformerSentenceEncoderLayer(\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJRgCW74m9wp"
      },
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFll3d14m_RF"
      },
      "source": [
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJjiYkZSmYko"
      },
      "source": [
        "def get_representation(model, tokenized_sentences):\n",
        "  tokenized_ids = [model.encode(sent) for sent in tokenized_sentences]\n",
        "  tokenized_tensor = [torch.tensor(item) for item in tokenized_ids]\n",
        "  encoded_list = []\n",
        "  for i in range(len(tokenized_tensor)):\n",
        "    with torch.no_grad():\n",
        "      encoded_layers = model.extract_features(tokenized_tensor[i])\n",
        "    encoded_list.append(encoded_layers)\n",
        "  sentence_list = [torch.mean(vec, dim=1).reshape((768)).numpy() for vec in encoded_list]\n",
        "  return sentence_list"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFiDYNRCnmQ3"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "def get_index_from_kmeans(sentence_embedding_list):\n",
        "    \"\"\"\n",
        "    Input a list of embeded sentence vectors\n",
        "    Output an list of indices of sentence in the paragraph, represent the clustering of key sentences\n",
        "    Note: Kmeans is used here for clustering\n",
        "    \"\"\"\n",
        "    n_clusters = np.ceil(len(sentence_embedding_list)**0.5)\n",
        "    kmeans = KMeans(n_clusters=int(n_clusters))\n",
        "    kmeans = kmeans.fit(sentence_embedding_list)\n",
        "    sum_index,_ = pairwise_distances_argmin_min(kmeans.cluster_centers_, sentence_embedding_list,metric='euclidean')\n",
        "    sum_index = sorted(sum_index)\n",
        "    return sum_index"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWRtOXMony_B"
      },
      "source": [
        "def get_summarization(text):\n",
        "  tokenized_sentences = sent_tokenize(text)\n",
        "  sentences_representation = get_representation(phoBERT, tokenized_sentences)\n",
        "  sum_index = get_index_from_kmeans(sentences_representation)\n",
        "  summary = ' '.join([tokenized_sentences[ind] for ind in sum_index])\n",
        "  return summary"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VY7XbjoXEd"
      },
      "source": [
        "text = '''\n",
        "Được phát triển dựa trên công nghệ AI nổi tiếng là Generative Adversarial Networks, phần mềm miễn phí này hoạt động nhờ hai mạng nơ-ron. Trong đó, mạng nơ-ron thứ hai có nhiệm vụ chỉ dẫn chi nơ-ron đầu tiên tái tạo màu của hình ảnh tốt hơn. Đây là một trong những công cụ chỉnh màu tốt nhất được các chuyên gia đánh giá. Nó cho ra kết quả màu sắc chân thực từ con người tới ngoại cảnh. Tuy nhiên\n",
        "\n",
        "DeOldify chạy trên Ubuntu, một bản phân phối Linux phổ biến. Do đó, để mang phần mềm này về máy tính của mình, người dùng sẽ cần có một số kiến thức công nghệ phù hợp để thực hiện được các thao tác.\n",
        "'''"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnXYI3tCobxF",
        "outputId": "81c3f5a5-8ef2-4637-ab2f-158125c8042a"
      },
      "source": [
        "print(get_summarization(text))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Được phát triển dựa trên công nghệ AI nổi tiếng là Generative Adversarial Networks, phần mềm miễn phí này hoạt động nhờ hai mạng nơ-ron. Trong đó, mạng nơ-ron thứ hai có nhiệm vụ chỉ dẫn chi nơ-ron đầu tiên tái tạo màu của hình ảnh tốt hơn. Đây là một trong những công cụ chỉnh màu tốt nhất được các chuyên gia đánh giá.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-UshY_MogW7"
      },
      "source": [
        "text = '''Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công_an quận Gò_Vấp , TPHCM đang điều_tra truy_xét vụ trộm xe ô_tô hiệu Ford_Ranger ở chung_cư Hà_Đô , phường 3 , quận Gò_Vấp .\n",
        "Thông_tin trên báo Thanh_Tra , Trung_tá Nguyễn_Đăng_Sơn , Phó Đội_trưởng đội Tổng_hợp , Công_an quận Gò_Vấp cho biết , theo thông_tin ban_đầu , vào_khoảng 10h ngày 13/3/2019 , anh Mai_Minh_N. ( sinh năm 1975 , tạm_trú tại lô D chung_cư Hà_Đô ) khi xuống lấy xe tại bãi giữ xe chung_cư Hà_Đô thì phát_hiện chiếc ô_tô hiệu Ford_Ranger , biển kiểm_soát : 51C-980.xx bị mất .\n",
        "Vụ_việc nhanh_chóng được trình_báo đến Công_an địa_phương .\n",
        "Nhận tin báo , Công_an quận Gò_Vấp có_mặt khám_nghiệm hiện_trường , lấy lời khai , điều_tra truy_xét .\n",
        "Qua trích xuất camera an_ninh , công_an xác_định lúc 21h26 ngày 12/3 , một thanh_niên ( chưa rõ lai_lịch ) đi_lại chiếc xe của anh N. gửi ở bãi xe .\n",
        "Sau đó , thanh_niên này mở_cửa , nổ máy chiếc xe ô_tô trên và rời khỏi bãi giữ xe .\n",
        "Hình_ảnh ghi lại đối_tượng thực_hiện vụ trộm_cắp khá mờ nên việc điều_tra truy_xét khó_khăn .\n",
        "Báo Tri_Thức_Trẻ thông_tin thêm , anh N. cho biết , chiếc ô_tô bị mất anh mua cách đây khoảng một năm , trị_giá gần 1 tỷ đồng , xe sắp hết hạn bảo_hiểm .\n",
        "Sáng ngày 13/3/2019 , khi anh xuống lấy xe để đi mua bảo_hiểm thì phát_hiện chiếc xe không còn ở bãi giữ xe .\n",
        "Hiện vụ_việc đang được điều_tra làm rõ .\n",
        "H.Y ( tổng_hợp )'''\n",
        "\n",
        "gold = '''Xuống nhà_xe chung_cư Hà_Đô lấy xe , một người_dân bàng_hoàng phát_hiện chiếc xế hộp tiền tỷ của mình sau một đêm đã không_cánh_mà_bay .'''"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjTg-zXorFga",
        "outputId": "41f26639-901d-49df-cf56-a24c4bc58259"
      },
      "source": [
        "summ = get_summarization(text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWWcuRmEst9f",
        "outputId": "529d7df5-ac04-478d-de19-cd78ae324f11"
      },
      "source": [
        "print(summ)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công_an quận Gò_Vấp , TPHCM đang điều_tra truy_xét vụ trộm xe ô_tô hiệu Ford_Ranger ở chung_cư Hà_Đô , phường 3 , quận Gò_Vấp . Qua trích xuất camera an_ninh , công_an xác_định lúc 21h26 ngày 12/3 , một thanh_niên ( chưa rõ lai_lịch ) đi_lại chiếc xe của anh N. gửi ở bãi xe . Hiện vụ_việc đang được điều_tra làm rõ . H.Y ( tổng_hợp )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kkRaCQSsP9Z",
        "outputId": "383603ac-7120-4822-e58b-44e6ef222a95"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFoeQ8uRsSuM"
      },
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo3AgBjysVyM",
        "outputId": "c13ae37d-5bda-497a-e93c-a1c8e1617012"
      },
      "source": [
        "rouge.get_scores(summ, gold)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.14583332968967022,\n",
              "   'p': 0.0958904109589041,\n",
              "   'r': 0.30434782608695654},\n",
              "  'rouge-2': {'f': 0.042553187904029274,\n",
              "   'p': 0.027777777777777776,\n",
              "   'r': 0.09090909090909091},\n",
              "  'rouge-l': {'f': 0.17948717543721246, 'p': 0.125, 'r': 0.3181818181818182}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssr4umb4tAsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}