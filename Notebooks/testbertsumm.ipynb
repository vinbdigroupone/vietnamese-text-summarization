{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testbertsumm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8RuvB2cExt5",
        "outputId": "82f6130a-fc4e-4704-aa50-be8e2de3b80d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 18 15:58:39 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOlNVhOlCGRU",
        "outputId": "8a2d6a50-3cd6-4af6-f3df-a0eedc036a60"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/31/c531a0cacd4f080e2a8b359248c2f52f4396fd5ae91a4eb7ded363c4f788/boto3-1.16.21-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.7)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.2MB/s \n",
            "\u001b[?25hCollecting botocore<1.20.0,>=1.19.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e7/bbf72c24d960735cf32503651941aee27daabb9c001dc06c0b8acb4db576/botocore-1.19.21-py2.py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 14.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.21->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.21->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.21 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.21 botocore-1.19.21 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaKcsoraopHb",
        "outputId": "6df9b755-9908-487f-e329-dfbd22f5ccde"
      },
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/5f/fddba88a1478e6223241065f779e3eb547e0a4db0a16ae46a2cf92a257b9/fairseq-0.10.0.tar.gz (677kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 8.7MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.7.0+cu101)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.7)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.21)\n",
            "Collecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/1f/7f502b9e37596164111655861370b08626f46f9e4524433c354f472765d4/hydra_core-1.0.4-py3-none-any.whl (122kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from hydra-core->fairseq) (3.3.0)\n",
            "Collecting omegaconf>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/f6/043b6d255dd6fbf2025110cea35b87f4c5100a181681d8eab496269f0d5b/omegaconf-2.0.5-py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 18.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 18.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.10.0-cp36-cp36m-linux_x86_64.whl size=2631870 sha256=e03c8185e967ad5ed62277b40236fc762824b84fadb174c1ecfb2261fd190135\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/23/f1/b5dba0d2da81577b3c426ef983e4bc021e5f751f35c5ba94a2\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime, PyYAML\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=c7bae07db4b151b4c5cbe836edb1760addd127edbb53169d25ee398ca05045a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=a70245a6b05e06f3d1e6f44ded92518befe1a16323d4fda0f9c1e95bdbc199e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built antlr4-python3-runtime PyYAML\n",
            "Installing collected packages: portalocker, sacrebleu, PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 fairseq-0.10.0 hydra-core-1.0.4 omegaconf-2.0.5 portalocker-2.0.0 sacrebleu-1.4.14\n",
            "Collecting fastbpe\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/37/f97181428a5d151501b90b2cebedf97c81b034ace753606a3cda5ad4e6e2/fastBPE-0.1.0.tar.gz\n",
            "Building wheels for collected packages: fastbpe\n",
            "  Building wheel for fastbpe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastbpe: filename=fastBPE-0.1.0-cp36-cp36m-linux_x86_64.whl size=481512 sha256=46dc564013284a53dd3b7726b4bcc23b5ea33bbd6153eb5e5e6357f10ed79c9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/0c/9c/fc62058b4d473a5602bcd3d3edfece796f123875379ea82d79\n",
            "Successfully built fastbpe\n",
            "Installing collected packages: fastbpe\n",
            "Successfully installed fastbpe-0.1.0\n",
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645934 sha256=aefa00b7ec0d71e8b761299837319c8c83b0a1714f4e4a30b5ab0c788eb7c23a\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=3b918ad361c45d873b5d702aa0bc1f343c37b9c1e303bfacb86badecb6a606f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGFckeXov5X",
        "outputId": "da9bf9d0-a210-43d3-935a-6324e635c4d7"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/22/1f5a4e4ae4dc318455cbbf2a154d557b28cc56437adce75583d2e22c38c3/underthesea-1.2.2-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from underthesea) (3.2.5)\n",
            "Collecting scikit-learn<0.22,>=0.20\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.17.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from underthesea) (7.1.2)\n",
            "Collecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from underthesea) (4.41.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from underthesea) (0.8.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn<0.22,>=0.20->underthesea) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->underthesea) (2.10)\n",
            "Installing collected packages: scikit-learn, python-crfsuite, unidecode, underthesea\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed python-crfsuite-0.9.7 scikit-learn-0.21.3 underthesea-1.2.2 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_It8DsSo6Wv",
        "outputId": "a5874753-aa14-42b5-9f77-1bd06783b0ef"
      },
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-19 02:48:55--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 99.86.35.72, 99.86.35.23, 99.86.35.98, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|99.86.35.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G  54.7MB/s    in 17s     \n",
            "\n",
            "2020-11-19 02:49:12 (70.3 MB/s) - ‘PhoBERT_base_fairseq.tar.gz’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFNvgtgGhmlp",
        "outputId": "9f513886-9365-4155-d698-4e4c69ac4447"
      },
      "source": [
        "!wget -O phobert.tar.gz https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
        "!tar -zxvf phobert.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-19 02:49:30--  https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 99.86.35.91, 99.86.35.98, 99.86.35.72, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|99.86.35.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322405979 (307M) [application/x-tar]\n",
            "Saving to: ‘phobert.tar.gz’\n",
            "\n",
            "phobert.tar.gz      100%[===================>] 307.47M  14.1MB/s    in 23s     \n",
            "\n",
            "2020-11-19 02:49:53 (13.5 MB/s) - ‘phobert.tar.gz’ saved [322405979/322405979]\n",
            "\n",
            "PhoBERT_base_transformers/\n",
            "PhoBERT_base_transformers/config.json\n",
            "PhoBERT_base_transformers/bpe.codes\n",
            "PhoBERT_base_transformers/model.bin\n",
            "PhoBERT_base_transformers/dict.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph7PxKxEo8OP"
      },
      "source": [
        "text1 = '''Ngày 27/3 , Cơ_quan Cảnh_sát điều_tra Công_an TP. Hưng_Yên , tỉnh Hưng_Yên cho biết , đơn_vị vừa ra quyết_định khởi_tố vụ án , khởi_tố bị_can đối_với đối_tượng Mai_Văn_Thương ( SN 1989 , trú tại đội 11 , thôn An_Chiểu 1 , xã Liên_Phương , TP. Hưng_Yên ) để điều_tra về hành_vi trộm_cắp tài_sản .\n",
        "Theo tài_liệu điều_tra của cơ_quan công_an , vào_khoảng 7h30 ngày 13/3 , lợi_dụng gia_đình ông Mai_Văn_Thịnh ( chú ruột đối_tượng Thương ) ở cạnh nhà đi vắng , đối_tượng này đã đạp gãy chấn_song cửa_sổ , đột_nhập vào nhà ông Thịnh trộm_cắp 121kg thóc mang bán cho người cùng thôn lấy 700.000 đ .\n",
        "Không dừng lại , sau đó đối_tượng tiếp_tục quay lại lục_soát tủ nhà ông Thịnh trộm_cắp 8.500.000 đ tiền_mặt ( ông Thịnh để dưới đáy tủ ) , rồi dùng số tiền trên để đi mua ma_tuý về sử_dụng và tiêu_xài hết 6.080.000 đ .\n",
        "Đến ngày 15/3 , đối_tượng Thương đã đến Cơ_quan điều_tra Công_an TP. Hưng_Yên tự_thú và khai nhận toàn_bộ hành_vi phạm_tội của mình , đồng_thời giao_nộp cho cơ_quan công_an 3.120.000 đ .\n",
        "Hiện Công_an TP. Hưng_Yên đã thu_giữ toàn_bộ 121kg thóc đối_tượng đã trộm_cắp để trao_trả cho gia_đình ông Thịnh .\n",
        "Được biết Thương là đối_tượng nghiện ma_tuý từ nhiều năm nay , đã có 1 tiền_án về tội Tàng_trữ trái_phép chất ma_tuý bị TAND tỉnh Hưng_Yên xử_phạt 2 năm 3 tháng tù_giam .\n",
        "Ra tù năm 2016 , đối_tượng này tiếp_tục có hành_vi cố_ý gây thương_tích , bị Công_an TP. Hưng_Yên ra quyết_định xử_phạt 2,5 triệu đồng .\n",
        "Vụ án đang được Công_an TP. Hưng_Yên hoàn_thiện hồ_sơ để xử_lý Mai_Văn_Thương theo quy_định của pháp_luật .'''\n",
        "\n",
        "summ1 = '''Với bản_tính ham chơi , lười làm , có nhiều tiền_án tiền_sự , lại nghiện ma_tuý , Thương đã đột_nhập vào nhà chú ruột để trộm hơn 1 tạ thóc và hơn 8 triệu đồng mang đi tiêu_xài .'''\n",
        "\n",
        "text2 = '''Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công_an quận Gò_Vấp , TPHCM đang điều_tra truy_xét vụ trộm xe ô_tô hiệu Ford_Ranger ở chung_cư Hà_Đô , phường 3 , quận Gò_Vấp .\n",
        "Thông_tin trên báo Thanh_Tra , Trung_tá Nguyễn_Đăng_Sơn , Phó Đội_trưởng đội Tổng_hợp , Công_an quận Gò_Vấp cho biết , theo thông_tin ban_đầu , vào_khoảng 10h ngày 13/3/2019 , anh Mai_Minh_N. ( sinh năm 1975 , tạm_trú tại lô D chung_cư Hà_Đô ) khi xuống lấy xe tại bãi giữ xe chung_cư Hà_Đô thì phát_hiện chiếc ô_tô hiệu Ford_Ranger , biển kiểm_soát : 51C-980.xx bị mất .\n",
        "Vụ_việc nhanh_chóng được trình_báo đến Công_an địa_phương .\n",
        "Nhận tin báo , Công_an quận Gò_Vấp có_mặt khám_nghiệm hiện_trường , lấy lời khai , điều_tra truy_xét .\n",
        "Qua trích xuất camera an_ninh , công_an xác_định lúc 21h26 ngày 12/3 , một thanh_niên ( chưa rõ lai_lịch ) đi_lại chiếc xe của anh N. gửi ở bãi xe .\n",
        "Sau đó , thanh_niên này mở_cửa , nổ máy chiếc xe ô_tô trên và rời khỏi bãi giữ xe .\n",
        "Hình_ảnh ghi lại đối_tượng thực_hiện vụ trộm_cắp khá mờ nên việc điều_tra truy_xét khó_khăn .\n",
        "Báo Tri_Thức_Trẻ thông_tin thêm , anh N. cho biết , chiếc ô_tô bị mất anh mua cách đây khoảng một năm , trị_giá gần 1 tỷ đồng , xe sắp hết hạn bảo_hiểm .\n",
        "Sáng ngày 13/3/2019 , khi anh xuống lấy xe để đi mua bảo_hiểm thì phát_hiện chiếc xe không còn ở bãi giữ xe .\n",
        "Hiện vụ_việc đang được điều_tra làm rõ .'''\n",
        "\n",
        "summ2 = '''Xuống nhà_xe chung_cư Hà_Đô lấy xe , một người_dân bàng_hoàng phát_hiện chiếc xế hộp tiền tỷ của mình sau một đêm đã không_cánh_mà_bay .'''\n",
        "\n",
        "text3 = '''Ngày 25/5 , đồn Công_an khu_công_nghiệp Vsip_Hải_Phòng Công_an huyện Thuỷ_Nguyên cho biết , đơn_vị vừa phối_hợp với văn_phòng cơ_quan CSĐT ( PC 01 ) Công_an TP. Hải_Phòng bắt_giữ và bàn_giao 2 đối_tượng Cà_Văn_Mái và Cà_Văn_Quân ( cùng SN 1993 , đều trú tại bản Thín B , xã Mường_Thín , huyện Tuần_Giáo , tỉnh Điện_Biên ) .\n",
        "Mái và Quân bị bắt_giữ theo quyết_định truy_nã số 04 và 05 ngày 22/9/2018 của Công_an huyện Tuần_Giáo , tỉnh Điện_Biên với tội_danh Cố_ý gây thương_tích .\n",
        "Trước đó , qua công_tác trao_đổi thông_tin , PC01 Công_an TP. Hải_Phòng tiếp_nhận thông_tin truy_nã về 2 đối_tượng Cà_Văn_Mái và Cà_Văn_Quân .\n",
        "Năm 2018 , Mái và Quân là 2 đối_tượng trong vụ án Cố_ý gây thương_tích bị Công_an huyện Tuần_Giáo , tỉnh Điện_Biên điều_tra , xử_lý .\n",
        "Sau khi gây án , 2 tên này đã nhanh chân bỏ trốn khỏi địa_phương và nhiều khả_năng đang lẩn_trốn tại khu_công_nghiệp Vsip_Hải_Phòng dưới vỏ bọc là những công_nhân xây_dựng .\n",
        "Để truy bắt thành_công các đối_tượng trốn nã , tổ công_tác đội 3 PC01 và đồn Công_an KCN Vsip đã phối_hợp chặt_chẽ , triển_khai đồng_bộ các biện_pháp nghiệp_vụ .\n",
        "Qua công_tác rà_soát địa_bàn , trinh_sát phát_hiện một nhóm người dân_tộc_thiểu_số đang làm công_nhân tại một công_trường xây_dựng trong KCN Vsip_Hải_Phòng , trong đó có 2 đối_tượng nghi_vấn là Mái và Quân .\n",
        "Để che_giấu thân_phận , 2 tên đã dùng “ chiêu ” thay tên đổi họ và khai_báo quê_quán ở các địa_phương khác .\n",
        "Hàng ngày , dưới “ vỏ bọc ” là những công_nhân , Mái cùng Quân sáng đi làm , tối về nhà_trọ đồng_thời luôn tỏ ra cần_cù , chịu_khó và hạn_chế giao_tiếp với những người xung_quanh .\n",
        "Dù vậy , sau khi bí_mật tiến_hành xác_minh , nhất_là “ dấu_hiệu ” giọng nói có phần lơ_lớ của đối_tượng , trinh_sát đã chắc_chắn xác_định : 2 công_nhân trên chính là 2 kẻ trốn truy_nã theo quyết_định truy_nã của Công_an huyện Tuần_Giáo , tỉnh Điện_Biên .\n",
        "Đến 18h30 ngày 16/5 , các lực_lượng công_an phối_hợp đã bắt gọn Mái và Quân .\n",
        "Tại cơ_quan công_an , sau một hồi quanh_co , 2 đối_tượng dưới vỏ bọc là những công_nhân chịu_khó đã phải cúi đầu thừa_nhận chính là Cà_Văn_Mái và Cà_Văn_Quân .\n",
        "Đồn Công_an KCN Vsip và PC01 Công_an TP. Hải_Phòng sau đó đã bàn_giao hồ_sơ và 2 đối_tượng cho Công_an tỉnh bạn xử_lý theo quy_định .\n",
        "'''\n",
        "summ3 = '''Sau khi gây án , bị công_an truy_nã gắt_gao , Mái và Quân đã thay tên đổi họ , xuống Hải_Phòng làm công_nhân và tỏ ra chăm_chỉ , hiền_lành .\n",
        "'''\n",
        "text4 = '''Sáng 20/10 , phòng Cảnh sát điều tra tội phạm về ma túy , Công_an tỉnh Đắk_Lắk cho biết , đang tiếp_tục điều_tra , xử_lý 21 nam_nữ thanh_niên tụ_tập trong quán karaoke sử_dụng ma_tuý bị lực_lượng công_an phát_hiện .\n",
        "Trước đó , vào khoảng 1h sáng 19/10 , tổ công tác của phòng Cảnh sát hình sự và phòng Cảnh sát cơ động , Công_an tỉnh Đắk Lắk tiến_hành kiểm tra quán karaoke GaLaXy ở 391 đường Hùng Vương , thị_xã Buôn_Hồ , tỉnh Đắk Lắk .\n",
        "Tại đây , tổ công_tác phát_hiện có 5 phòng đang hát .\n",
        "Trong đó , 4 phòng hát có 22 thanh_niên nam , nữ đang có biểu hiện phê ma túy , bật nhạc to và nhảy múa .\n",
        "Thời_điểm kiểm_tra tại 4 phòng hát này , tổ công tác thu giữ nhiều tang vật như : Ma túy đá , ketamin dùng để hít , thuốc lắc và cỏ Mỹ .\n",
        "Lúc này , tổ công_tác đã đưa các đối_tượng trên về bệnh viện Đa_khoa TP. Buôn_Ma Thuột , tỉnh Đắk_Lắk để kiểm_tra .\n",
        "Kết_quả cho thấy , có 21 đối tượng gồm 6 nữ và 15 nam dương tính với chất ma túy .\n",
        "Tại cơ_quan công_an , các đối tượng khai nhận đã mua các chất ma túy trên rồi rủ nhau đi hát karaoke để tổ chức sử dụng .\n",
        "Hiện , vụ_việc đang được phòng Cảnh sát điều tra tội phạm về ma túy Công_an tỉnh Đắk Lắk tiếp_tục điều_tra , xử lý theo quy định của pháp luật .'''\n",
        "summ4 = '''Khi ập vào kiểm_tra quán karaoke , công_an phát_hiện có hàng chục nam_nữ thanh_niên với biểu_hiện phê ma_tuý , nhảy_nhót , lắc_lư trong tiếng nhạc chát_chúa .'''\n",
        "\n",
        "text5 = '''Ngày 26/10 , đại_diện Công_an quận Hải_Châu , TP. Đà_Nẵng cho biết , đang làm_việc với Võ_Đức_Thắng ( 30 tuổi ) và Nguyễn_Quỳnh_Thy ( 27 tuổi ) , ngụ tại địa_phương về hành_vi vượt đèn_đỏ rồi đạp ngã 2 cảnh_sát đang làm nhiệm_vụ .\n",
        "Vào sáng cùng ngày , Thắng và Thy điều_khiển xe_máy hiệu Exciter trên đường Quang_Trung .\n",
        "Khi đến đoạn giao với Nguyễn_Thị_Minh_Khai , quận Hải_Châu thì Thắng rú ga vượt đèn_đỏ .\n",
        "Lúc này , tổ Cảnh_sát Trật_tự , thuộc đội Cảnh_sát giao_thông trật_tự cơ_động Công_an quận Hải_Châu , gồm 4 cán_bộ , chiến_sĩ chạy mô_tô làm_việc tại đây thấy vậy liền truy_đuổi .\n",
        "Biết cơ_quan_chức_năng đuổi theo , Thắng không dừng xe mà rú ga với tốc_độ nhanh hơn .\n",
        "Các chiến_sĩ đuổi kịp , áp sát chặn xe thì Thắng dùng chân đạp vào xe mô_tô khiến 2 chiến_sĩ công_an ngã .\n",
        "2 chiến_sĩ còn lại tiếp_tục truy_đuổi .\n",
        "Khi đến ngã tư Quang_Trung – Nguyễn_Chí_Thanh , đôi nam_nữ bị chặn lại .\n",
        "Vụ_việc khiến 1 chiến_sĩ bị_thương ở cổ_tay , khuỷu tay , xe mô_tô chuyên_dụng bị hư_hỏng phần đầu .\n",
        "Hiện , cơ_quan_chức_năng đang tiếp_tục điều_tra , làm rõ vụ_việc .'''\n",
        "\n",
        "summ5 = '''Thắng chở Thy vượt đèn_đỏ , khi biết có chiến_sĩ công_an đuổi theo , Thắng rồ ga chạy nhanh . Khi các chiến_sĩ đuổi kịp , Thắng dùng chân đạp ngã xe khiến 1 chiến_sĩ bị_thương .'''\n",
        "\n",
        "text6 = '''Ngày 25/3 , trao_đổi với PV báo Người Đưa_Tin , luật_sư Đặng_Thị_Vân_Thịnh – Văn_phòng luật_sư Kết_Nối , đồng_thời là người bảo_vệ quyền và lợi_ích hợp_pháp cho cháu bé 9 tuổi ở Chương_Mỹ bị xâm_hại cho_hay , mới_đây VKSND TP. Hà_Nội đã phê_chuẩn quyết_định chuyển tội_danh Dâm_ô với người dưới 16 tuổi do Công_an huyện Chương_Mỹ khởi_tố sang tội_danh Hiếp_dâm người dưới 16 tuổi đối_với bị_can Nguyễn_Trọng_Trình ( 31 tuổi , trú xã Hoà_Chính , huyện Chương_Mỹ ) .\n",
        "Liên_quan đến quyết_định phê_chuẩn của VKS về việc thay_đổi quyết_định khởi_tố bị_can , thay_đổi tội_danh , luật_sư Thịnh cho_rằng việc_làm này là rất kịp_thời và phù_hợp với quy_định của pháp_luật ; góp_phần xoa_dịu những nỗi_đau mà bị hại và gia_đình bị hại đã phải trải qua ; cũng như củng_cố thêm niềm_tin của quần_chúng nhân_dân vào các cơ_quan tiến_hành tố_tụng , vào công_lý .\n",
        "Đồng_thời qua đó cũng tạo sức răn_đe , cảnh_báo , là bài_học lớn cho những đối_tượng có tư_tưởng , hành_vi xâm_hại trẻ_em .\n",
        "Là người bảo_vệ cho cháu bé , lại là một người phụ_nữ , luật_sư Thịnh rất thấu_hiểu những nỗi_đau mà cháu bé đang và có_thể sẽ phải gánh_chịu trong tương_lai .\n",
        "Cháu bé ở độ tuổi tâm_sinh_lý đang phát_triển .\n",
        "Hành_vi mất nhân_tính của bị_can đã gây những ám_ảnh khủng_khiếp cho cháu bé không_chỉ thời_điểm hiện_tại mà có_thể sẽ còn đeo_bám cháu đến những chuỗi ngày về sau .\n",
        "Do_vậy , luật_sư Thịnh cũng rất đau_đáu , trăn_trở về trường_hợp của cháu bé , nếu cháu không kịp_thời được sự quan_tâm , động_viên , chia_sẻ , cảm_thông của gia_đình , bạn_bè và xã_hội thì sẽ dẫn đến những khủng_hoảng về tâm_lý cho cháu .\n",
        "Đánh_giá về tội_danh của bị_can , luật_sư Thịnh cho rằng : Hành_vi của Trình dùng vũ_lực đối_với cháu ( có hậu_quả là cháu bị gãy răng , rạn tay .. ) ; lợi_dụng tình_trạng không_thể tự_vệ được của cháu ( cháu là trẻ_em , trong khi Trình là một thanh_niên to_khoẻ ) để giao_cấu hoặc thực_hiện hành_vi quan_hệ tình_dục khác ( hậu_quả làm cháu bị rách màng_trinh và thủng tầng sinh_môn ) như_vậy đã đầy_đủ yếu_tố để cấu_thành tội Hiếp_dâm người dưới 16 tuổi .\n",
        "Theo thông_tin gia_đình nạn_nhân cung_cấp , thời_điểm cháu Q. bị xâm_hại mới được 9 tuổi , 5 tháng 20 ngày ( tức_là dưới 10 tuổi ) .\n",
        "Chiếu theo quy_định tại Điều 142 Bộ luật_Hình_sự năm 2015 , sửa_đổi bổ_sung năm 2017 về tội Hiếp_dâm người dưới 16 tuổi thì Trình sẽ bị khởi_tố ở khoản 3 điểm c là \" Phạm_tội đối_với người dưới 10 tuổi ” .\n",
        "“ Đối_tượng phạm_tội thuộc trường_hợp này thì bị phạt tù 20 năm , tù chung_thân hoặc tử_hình ” , luật_sư Thịnh cho biết .'''\n",
        "\n",
        "summ6 = '''Trong trường_hợp bị chuyển tội_danh từ Dâm_ô với người dưới 16 tuổi sang tội_danh Hiếp_dâm người dưới 16 tuổi , gã “ đồ_tể ” Nguyễn_Trọng_Trình có_thể đối_mặt với mức án cao nhất là tử_hình .'''\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "tDwLjD9Op5VF",
        "outputId": "8aca98bb-a63d-4e3a-a0bb-374abd98f378"
      },
      "source": [
        "import pandas as pd\n",
        "data = {'content': [text1, text2, text3, text4, text5, text6], 'summary': [summ1, summ2, summ3, summ4, summ5, summ6]}\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ngày 27/3 , Cơ_quan Cảnh_sát điều_tra Công_an ...</td>\n",
              "      <td>Với bản_tính ham chơi , lười làm , có nhiều ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công...</td>\n",
              "      <td>Xuống nhà_xe chung_cư Hà_Đô lấy xe , một người...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ngày 25/5 , đồn Công_an khu_công_nghiệp Vsip_H...</td>\n",
              "      <td>Sau khi gây án , bị công_an truy_nã gắt_gao , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sáng 20/10 , phòng Cảnh sát điều tra tội ...</td>\n",
              "      <td>Khi ập vào kiểm_tra quán karaoke , công_an phá...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ngày 26/10 , đại_diện Công_an quận Hải_Châu , ...</td>\n",
              "      <td>Thắng chở Thy vượt đèn_đỏ , khi biết có chiến_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content                                            summary\n",
              "0  Ngày 27/3 , Cơ_quan Cảnh_sát điều_tra Công_an ...  Với bản_tính ham chơi , lười làm , có nhiều ti...\n",
              "1  Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công...  Xuống nhà_xe chung_cư Hà_Đô lấy xe , một người...\n",
              "2  Ngày 25/5 , đồn Công_an khu_công_nghiệp Vsip_H...  Sau khi gây án , bị công_an truy_nã gắt_gao , ...\n",
              "3  Sáng 20/10 , phòng Cảnh sát điều tra tội ...  Khi ập vào kiểm_tra quán karaoke , công_an phá...\n",
              "4  Ngày 26/10 , đại_diện Công_an quận Hải_Châu , ...  Thắng chở Thy vượt đèn_đỏ , khi biết có chiến_..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqLr-jviqRwf",
        "outputId": "904c116f-93e3-4466-ea4c-b3d2c1442bf6"
      },
      "source": [
        "def get_content_summary_from_df(df):\n",
        "    content = df['content'].values.tolist()\n",
        "    summary = df['summary'].values.tolist()\n",
        "    return content, summary\n",
        "\n",
        "content, summary = get_content_summary_from_df(df)\n",
        "print(content)\n",
        "print(summary)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ngày 27/3 , Cơ_quan Cảnh_sát điều_tra Công_an TP. Hưng_Yên , tỉnh Hưng_Yên cho biết , đơn_vị vừa ra quyết_định khởi_tố vụ án , khởi_tố bị_can đối_với đối_tượng Mai_Văn_Thương ( SN 1989 , trú tại đội 11 , thôn An_Chiểu 1 , xã Liên_Phương , TP. Hưng_Yên ) để điều_tra về hành_vi trộm_cắp tài_sản .\\nTheo tài_liệu điều_tra của cơ_quan công_an , vào_khoảng 7h30 ngày 13/3 , lợi_dụng gia_đình ông Mai_Văn_Thịnh ( chú ruột đối_tượng Thương ) ở cạnh nhà đi vắng , đối_tượng này đã đạp gãy chấn_song cửa_sổ , đột_nhập vào nhà ông Thịnh trộm_cắp 121kg thóc mang bán cho người cùng thôn lấy 700.000 đ .\\nKhông dừng lại , sau đó đối_tượng tiếp_tục quay lại lục_soát tủ nhà ông Thịnh trộm_cắp 8.500.000 đ tiền_mặt ( ông Thịnh để dưới đáy tủ ) , rồi dùng số tiền trên để đi mua ma_tuý về sử_dụng và tiêu_xài hết 6.080.000 đ .\\nĐến ngày 15/3 , đối_tượng Thương đã đến Cơ_quan điều_tra Công_an TP. Hưng_Yên tự_thú và khai nhận toàn_bộ hành_vi phạm_tội của mình , đồng_thời giao_nộp cho cơ_quan công_an 3.120.000 đ .\\nHiện Công_an TP. Hưng_Yên đã thu_giữ toàn_bộ 121kg thóc đối_tượng đã trộm_cắp để trao_trả cho gia_đình ông Thịnh .\\nĐược biết Thương là đối_tượng nghiện ma_tuý từ nhiều năm nay , đã có 1 tiền_án về tội Tàng_trữ trái_phép chất ma_tuý bị TAND tỉnh Hưng_Yên xử_phạt 2 năm 3 tháng tù_giam .\\nRa tù năm 2016 , đối_tượng này tiếp_tục có hành_vi cố_ý gây thương_tích , bị Công_an TP. Hưng_Yên ra quyết_định xử_phạt 2,5 triệu đồng .\\nVụ án đang được Công_an TP. Hưng_Yên hoàn_thiện hồ_sơ để xử_lý Mai_Văn_Thương theo quy_định của pháp_luật .', 'Theo báo Sài_Gòn Giải_Phóng , ngày 17/3 , Công_an quận Gò_Vấp , TPHCM đang điều_tra truy_xét vụ trộm xe ô_tô hiệu Ford_Ranger ở chung_cư Hà_Đô , phường 3 , quận Gò_Vấp .\\nThông_tin trên báo Thanh_Tra , Trung_tá Nguyễn_Đăng_Sơn , Phó Đội_trưởng đội Tổng_hợp , Công_an quận Gò_Vấp cho biết , theo thông_tin ban_đầu , vào_khoảng 10h ngày 13/3/2019 , anh Mai_Minh_N. ( sinh năm 1975 , tạm_trú tại lô D chung_cư Hà_Đô ) khi xuống lấy xe tại bãi giữ xe chung_cư Hà_Đô thì phát_hiện chiếc ô_tô hiệu Ford_Ranger , biển kiểm_soát : 51C-980.xx bị mất .\\nVụ_việc nhanh_chóng được trình_báo đến Công_an địa_phương .\\nNhận tin báo , Công_an quận Gò_Vấp có_mặt khám_nghiệm hiện_trường , lấy lời khai , điều_tra truy_xét .\\nQua trích xuất camera an_ninh , công_an xác_định lúc 21h26 ngày 12/3 , một thanh_niên ( chưa rõ lai_lịch ) đi_lại chiếc xe của anh N. gửi ở bãi xe .\\nSau đó , thanh_niên này mở_cửa , nổ máy chiếc xe ô_tô trên và rời khỏi bãi giữ xe .\\nHình_ảnh ghi lại đối_tượng thực_hiện vụ trộm_cắp khá mờ nên việc điều_tra truy_xét khó_khăn .\\nBáo Tri_Thức_Trẻ thông_tin thêm , anh N. cho biết , chiếc ô_tô bị mất anh mua cách đây khoảng một năm , trị_giá gần 1 tỷ đồng , xe sắp hết hạn bảo_hiểm .\\nSáng ngày 13/3/2019 , khi anh xuống lấy xe để đi mua bảo_hiểm thì phát_hiện chiếc xe không còn ở bãi giữ xe .\\nHiện vụ_việc đang được điều_tra làm rõ .', 'Ngày 25/5 , đồn Công_an khu_công_nghiệp Vsip_Hải_Phòng Công_an huyện Thuỷ_Nguyên cho biết , đơn_vị vừa phối_hợp với văn_phòng cơ_quan CSĐT ( PC 01 ) Công_an TP. Hải_Phòng bắt_giữ và bàn_giao 2 đối_tượng Cà_Văn_Mái và Cà_Văn_Quân ( cùng SN 1993 , đều trú tại bản Thín B , xã Mường_Thín , huyện Tuần_Giáo , tỉnh Điện_Biên ) .\\nMái và Quân bị bắt_giữ theo quyết_định truy_nã số 04 và 05 ngày 22/9/2018 của Công_an huyện Tuần_Giáo , tỉnh Điện_Biên với tội_danh Cố_ý gây thương_tích .\\nTrước đó , qua công_tác trao_đổi thông_tin , PC01 Công_an TP. Hải_Phòng tiếp_nhận thông_tin truy_nã về 2 đối_tượng Cà_Văn_Mái và Cà_Văn_Quân .\\nNăm 2018 , Mái và Quân là 2 đối_tượng trong vụ án Cố_ý gây thương_tích bị Công_an huyện Tuần_Giáo , tỉnh Điện_Biên điều_tra , xử_lý .\\nSau khi gây án , 2 tên này đã nhanh chân bỏ trốn khỏi địa_phương và nhiều khả_năng đang lẩn_trốn tại khu_công_nghiệp Vsip_Hải_Phòng dưới vỏ bọc là những công_nhân xây_dựng .\\nĐể truy bắt thành_công các đối_tượng trốn nã , tổ công_tác đội 3 PC01 và đồn Công_an KCN Vsip đã phối_hợp chặt_chẽ , triển_khai đồng_bộ các biện_pháp nghiệp_vụ .\\nQua công_tác rà_soát địa_bàn , trinh_sát phát_hiện một nhóm người dân_tộc_thiểu_số đang làm công_nhân tại một công_trường xây_dựng trong KCN Vsip_Hải_Phòng , trong đó có 2 đối_tượng nghi_vấn là Mái và Quân .\\nĐể che_giấu thân_phận , 2 tên đã dùng “ chiêu ” thay tên đổi họ và khai_báo quê_quán ở các địa_phương khác .\\nHàng ngày , dưới “ vỏ bọc ” là những công_nhân , Mái cùng Quân sáng đi làm , tối về nhà_trọ đồng_thời luôn tỏ ra cần_cù , chịu_khó và hạn_chế giao_tiếp với những người xung_quanh .\\nDù vậy , sau khi bí_mật tiến_hành xác_minh , nhất_là “ dấu_hiệu ” giọng nói có phần lơ_lớ của đối_tượng , trinh_sát đã chắc_chắn xác_định : 2 công_nhân trên chính là 2 kẻ trốn truy_nã theo quyết_định truy_nã của Công_an huyện Tuần_Giáo , tỉnh Điện_Biên .\\nĐến 18h30 ngày 16/5 , các lực_lượng công_an phối_hợp đã bắt gọn Mái và Quân .\\nTại cơ_quan công_an , sau một hồi quanh_co , 2 đối_tượng dưới vỏ bọc là những công_nhân chịu_khó đã phải cúi đầu thừa_nhận chính là Cà_Văn_Mái và Cà_Văn_Quân .\\nĐồn Công_an KCN Vsip và PC01 Công_an TP. Hải_Phòng sau đó đã bàn_giao hồ_sơ và 2 đối_tượng cho Công_an tỉnh bạn xử_lý theo quy_định .\\n', 'Sáng 20/10 , phòng Cảnh sát điều tra tội phạm về ma túy , Công_an tỉnh Đắk_Lắk cho biết , đang tiếp_tục điều_tra , xử_lý 21 nam_nữ thanh_niên tụ_tập trong quán karaoke sử_dụng ma_tuý bị lực_lượng công_an phát_hiện .\\nTrước đó , vào khoảng 1h sáng 19/10 , tổ công tác của phòng Cảnh sát hình sự và phòng Cảnh sát cơ động , Công_an tỉnh Đắk Lắk tiến_hành kiểm tra quán karaoke GaLaXy ở 391 đường Hùng Vương , thị_xã Buôn_Hồ , tỉnh Đắk Lắk .\\nTại đây , tổ công_tác phát_hiện có 5 phòng đang hát .\\nTrong đó , 4 phòng hát có 22 thanh_niên nam , nữ đang có biểu hiện phê ma túy , bật nhạc to và nhảy múa .\\nThời_điểm kiểm_tra tại 4 phòng hát này , tổ công tác thu giữ nhiều tang vật như : Ma túy đá , ketamin dùng để hít , thuốc lắc và cỏ Mỹ .\\nLúc này , tổ công_tác đã đưa các đối_tượng trên về bệnh viện Đa_khoa TP. Buôn_Ma Thuột , tỉnh Đắk_Lắk để kiểm_tra .\\nKết_quả cho thấy , có 21 đối tượng gồm 6 nữ và 15 nam dương tính với chất ma túy .\\nTại cơ_quan công_an , các đối tượng khai nhận đã mua các chất ma túy trên rồi rủ nhau đi hát karaoke để tổ chức sử dụng .\\nHiện , vụ_việc đang được phòng Cảnh sát điều tra tội phạm về ma túy Công_an tỉnh Đắk Lắk tiếp_tục điều_tra , xử lý theo quy định của pháp luật .', 'Ngày 26/10 , đại_diện Công_an quận Hải_Châu , TP. Đà_Nẵng cho biết , đang làm_việc với Võ_Đức_Thắng ( 30 tuổi ) và Nguyễn_Quỳnh_Thy ( 27 tuổi ) , ngụ tại địa_phương về hành_vi vượt đèn_đỏ rồi đạp ngã 2 cảnh_sát đang làm nhiệm_vụ .\\nVào sáng cùng ngày , Thắng và Thy điều_khiển xe_máy hiệu Exciter trên đường Quang_Trung .\\nKhi đến đoạn giao với Nguyễn_Thị_Minh_Khai , quận Hải_Châu thì Thắng rú ga vượt đèn_đỏ .\\nLúc này , tổ Cảnh_sát Trật_tự , thuộc đội Cảnh_sát giao_thông trật_tự cơ_động Công_an quận Hải_Châu , gồm 4 cán_bộ , chiến_sĩ chạy mô_tô làm_việc tại đây thấy vậy liền truy_đuổi .\\nBiết cơ_quan_chức_năng đuổi theo , Thắng không dừng xe mà rú ga với tốc_độ nhanh hơn .\\nCác chiến_sĩ đuổi kịp , áp sát chặn xe thì Thắng dùng chân đạp vào xe mô_tô khiến 2 chiến_sĩ công_an ngã .\\n2 chiến_sĩ còn lại tiếp_tục truy_đuổi .\\nKhi đến ngã tư Quang_Trung – Nguyễn_Chí_Thanh , đôi nam_nữ bị chặn lại .\\nVụ_việc khiến 1 chiến_sĩ bị_thương ở cổ_tay , khuỷu tay , xe mô_tô chuyên_dụng bị hư_hỏng phần đầu .\\nHiện , cơ_quan_chức_năng đang tiếp_tục điều_tra , làm rõ vụ_việc .', 'Ngày 25/3 , trao_đổi với PV báo Người Đưa_Tin , luật_sư Đặng_Thị_Vân_Thịnh – Văn_phòng luật_sư Kết_Nối , đồng_thời là người bảo_vệ quyền và lợi_ích hợp_pháp cho cháu bé 9 tuổi ở Chương_Mỹ bị xâm_hại cho_hay , mới_đây VKSND TP. Hà_Nội đã phê_chuẩn quyết_định chuyển tội_danh Dâm_ô với người dưới 16 tuổi do Công_an huyện Chương_Mỹ khởi_tố sang tội_danh Hiếp_dâm người dưới 16 tuổi đối_với bị_can Nguyễn_Trọng_Trình ( 31 tuổi , trú xã Hoà_Chính , huyện Chương_Mỹ ) .\\nLiên_quan đến quyết_định phê_chuẩn của VKS về việc thay_đổi quyết_định khởi_tố bị_can , thay_đổi tội_danh , luật_sư Thịnh cho_rằng việc_làm này là rất kịp_thời và phù_hợp với quy_định của pháp_luật ; góp_phần xoa_dịu những nỗi_đau mà bị hại và gia_đình bị hại đã phải trải qua ; cũng như củng_cố thêm niềm_tin của quần_chúng nhân_dân vào các cơ_quan tiến_hành tố_tụng , vào công_lý .\\nĐồng_thời qua đó cũng tạo sức răn_đe , cảnh_báo , là bài_học lớn cho những đối_tượng có tư_tưởng , hành_vi xâm_hại trẻ_em .\\nLà người bảo_vệ cho cháu bé , lại là một người phụ_nữ , luật_sư Thịnh rất thấu_hiểu những nỗi_đau mà cháu bé đang và có_thể sẽ phải gánh_chịu trong tương_lai .\\nCháu bé ở độ tuổi tâm_sinh_lý đang phát_triển .\\nHành_vi mất nhân_tính của bị_can đã gây những ám_ảnh khủng_khiếp cho cháu bé không_chỉ thời_điểm hiện_tại mà có_thể sẽ còn đeo_bám cháu đến những chuỗi ngày về sau .\\nDo_vậy , luật_sư Thịnh cũng rất đau_đáu , trăn_trở về trường_hợp của cháu bé , nếu cháu không kịp_thời được sự quan_tâm , động_viên , chia_sẻ , cảm_thông của gia_đình , bạn_bè và xã_hội thì sẽ dẫn đến những khủng_hoảng về tâm_lý cho cháu .\\nĐánh_giá về tội_danh của bị_can , luật_sư Thịnh cho rằng : Hành_vi của Trình dùng vũ_lực đối_với cháu ( có hậu_quả là cháu bị gãy răng , rạn tay .. ) ; lợi_dụng tình_trạng không_thể tự_vệ được của cháu ( cháu là trẻ_em , trong khi Trình là một thanh_niên to_khoẻ ) để giao_cấu hoặc thực_hiện hành_vi quan_hệ tình_dục khác ( hậu_quả làm cháu bị rách màng_trinh và thủng tầng sinh_môn ) như_vậy đã đầy_đủ yếu_tố để cấu_thành tội Hiếp_dâm người dưới 16 tuổi .\\nTheo thông_tin gia_đình nạn_nhân cung_cấp , thời_điểm cháu Q. bị xâm_hại mới được 9 tuổi , 5 tháng 20 ngày ( tức_là dưới 10 tuổi ) .\\nChiếu theo quy_định tại Điều 142 Bộ luật_Hình_sự năm 2015 , sửa_đổi bổ_sung năm 2017 về tội Hiếp_dâm người dưới 16 tuổi thì Trình sẽ bị khởi_tố ở khoản 3 điểm c là \" Phạm_tội đối_với người dưới 10 tuổi ” .\\n“ Đối_tượng phạm_tội thuộc trường_hợp này thì bị phạt tù 20 năm , tù chung_thân hoặc tử_hình ” , luật_sư Thịnh cho biết .']\n",
            "['Với bản_tính ham chơi , lười làm , có nhiều tiền_án tiền_sự , lại nghiện ma_tuý , Thương đã đột_nhập vào nhà chú ruột để trộm hơn 1 tạ thóc và hơn 8 triệu đồng mang đi tiêu_xài .', 'Xuống nhà_xe chung_cư Hà_Đô lấy xe , một người_dân bàng_hoàng phát_hiện chiếc xế hộp tiền tỷ của mình sau một đêm đã không_cánh_mà_bay .', 'Sau khi gây án , bị công_an truy_nã gắt_gao , Mái và Quân đã thay tên đổi họ , xuống Hải_Phòng làm công_nhân và tỏ ra chăm_chỉ , hiền_lành .\\n', 'Khi ập vào kiểm_tra quán karaoke , công_an phát_hiện có hàng chục nam_nữ thanh_niên với biểu_hiện phê ma_tuý , nhảy_nhót , lắc_lư trong tiếng nhạc chát_chúa .', 'Thắng chở Thy vượt đèn_đỏ , khi biết có chiến_sĩ công_an đuổi theo , Thắng rồ ga chạy nhanh . Khi các chiến_sĩ đuổi kịp , Thắng dùng chân đạp ngã xe khiến 1 chiến_sĩ bị_thương .', 'Trong trường_hợp bị chuyển tội_danh từ Dâm_ô với người dưới 16 tuổi sang tội_danh Hiếp_dâm người dưới 16 tuổi , gã “ đồ_tể ” Nguyễn_Trọng_Trình có_thể đối_mặt với mức án cao nhất là tử_hình .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc-7BcP4qgop"
      },
      "source": [
        "from underthesea import sent_tokenize\n",
        "from underthesea import word_tokenize"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "oZeO9DYHqlQy",
        "outputId": "3bc44762-171a-4c3e-b777-102bfb21a98b"
      },
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "# Khởi tạo Byte Pair Encoding cho PhoBERT\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "tokens = phoBERT.encode('Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh')\n",
        "print('tokens list : ', tokens)\n",
        "# Decode ngược lại thành câu từ chuỗi index token\n",
        "phoBERT.decode(tokens)  # 'Hello world!'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-19 02:50:03 | INFO | fairseq.file_utils | loading archive file PhoBERT_base_fairseq\n",
            "2020-11-19 02:50:04 | INFO | fairseq.tasks.masked_lm | dictionary: 64000 types\n",
            "2020-11-19 02:50:09 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json not found in cache, downloading to /tmp/tmp5r0mx8hl\n",
            "1042301B [00:00, 3855934.34B/s]\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | copying /tmp/tmp5r0mx8hl to cache at /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/e2aab4d600e7568c2d88fc7732130ccc815ea84ec63906cb0913c7a3a4906a2e.0f323dfaed92d080380e63f0291d0f31adfa8c61a62cbcb3cb8114f061be27f7\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | removing temp file /tmp/tmp5r0mx8hl\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe not found in cache, downloading to /tmp/tmpcd8m3i51\n",
            "456318B [00:00, 3768741.27B/s]\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | copying /tmp/tmpcd8m3i51 to cache at /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | creating metadata file for /root/.cache/torch/pytorch_fairseq/b04a6d337c09f464fe8f0df1d3524db88a597007d63f05d97e437f65840cdba5.939bed25cbdab15712bac084ee713d6c78e221c5156c68cb0076b03f5170600f\n",
            "2020-11-19 02:50:10 | INFO | fairseq.file_utils | removing temp file /tmp/tmpcd8m3i51\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tokens list :  tensor([    0, 11623, 31433,   453, 44334,  2080,  5922,    57,   934,  8181,\n",
            "        31686,  3078,     2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU13aPJ6rqU8"
      },
      "source": [
        "tokenized_content = []\n",
        "for sent in content:\n",
        "  temp = phoBERT.encode(sent)\n",
        "  temp = temp.tolist()\n",
        "  tokenized_content.append(temp)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdmkRc25Z9o"
      },
      "source": [
        "tokenized_summary = []\n",
        "for sent in summary:\n",
        "  temp = phoBERT.encode(sent)\n",
        "  temp = temp.tolist()\n",
        "  tokenized_summary.append(temp)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yDU9qZ95iuA",
        "outputId": "433a86d4-27d9-4b39-accb-b9165ff112bc"
      },
      "source": [
        "print(type(tokenized_summary))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnVVNKtjsu6N"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-FEW0LXscm9"
      },
      "source": [
        "data = {'train': {\n",
        "            'src': tokenized_content[:4],\n",
        "            'tgt': tokenized_summary[:4]},\n",
        "        'valid': {\n",
        "            'src': tokenized_content[4:],\n",
        "            'tgt': tokenized_summary[4:]}}\n",
        "torch.save(data, '/content/data.pt')            "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm8Ki0S3t6nO"
      },
      "source": [
        "PAD = 0\n",
        "UNK = 1\n",
        "BOS = 2\n",
        "EOS = 3\n",
        "\n",
        "PAD_WORD = '[PAD]'\n",
        "UNK_WORD = '[UNK]'\n",
        "BOS_WORD = '[CLS]'\n",
        "EOS_WORD = '[SEP]'\n",
        "\n",
        "max_src_size = 512\n",
        "max_tgt_size = 512"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7YHpveosth8"
      },
      "source": [
        "import torch.utils.data"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FUwAP8Dvkgx"
      },
      "source": [
        "max_src_size = 768\n",
        "max_tgt_size = 768"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxpDN48itO9q"
      },
      "source": [
        "class TextSummarizationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            src_insts=None, tgt_insts=None):\n",
        "\n",
        "        assert src_insts\n",
        "        assert not tgt_insts or (len(src_insts) == len(tgt_insts))\n",
        "\n",
        "        # src_idx2word = {idx:word for word, idx in src_word2idx.items()}\n",
        "        # self._src_word2idx = src_word2idx\n",
        "        # self._src_idx2word = src_idx2word\n",
        "        self._src_insts = src_insts\n",
        "\n",
        "        # tgt_idx2word = {idx:word for word, idx in tgt_word2idx.items()}\n",
        "        # self._tgt_word2idx = tgt_word2idx\n",
        "        # self._tgt_idx2word = tgt_idx2word\n",
        "        self._tgt_insts = tgt_insts\n",
        "\n",
        "    @property\n",
        "    def n_insts(self):\n",
        "        \"\"\" Property for dataset size \"\"\"\n",
        "        return len(self._src_insts)\n",
        "\n",
        "    # @property\n",
        "    # def src_vocab_size(self):\n",
        "    #     \"\"\" Property for vocab size \"\"\"\n",
        "    #     return len(self._src_word2idx)\n",
        "\n",
        "    # @property\n",
        "    # def tgt_vocab_size(self):\n",
        "    #     \"\"\" Property for vocab size \"\"\"\n",
        "    #     return len(self._tgt_word2idx)\n",
        "\n",
        "    # @property\n",
        "    # def src_word2idx(self):\n",
        "    #     \"\"\" Property for word dictionary \"\"\"\n",
        "    #     return self._src_word2idx\n",
        "\n",
        "    # @property\n",
        "    # def tgt_word2idx(self):\n",
        "    #     \"\"\" Property for word dictionary \"\"\"\n",
        "    #     return self._tgt_word2idx\n",
        "\n",
        "    # @property\n",
        "    # def src_idx2word(self):\n",
        "    #     \"\"\" Property for index dictionary \"\"\"\n",
        "    #     return self._src_idx2word\n",
        "\n",
        "    # @property\n",
        "    # def tgt_idx2word(self):\n",
        "    #     \"\"\" Property for index dictionary \"\"\"\n",
        "    #     return self._tgt_idx2word\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_insts\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self._tgt_insts:\n",
        "            return self._src_insts[idx], self._tgt_insts[idx]\n",
        "        return self._src_insts[idx]\n",
        "\n",
        "\n",
        "def paired_collate_fn(insts):\n",
        "    src_insts, tgt_insts = list(zip(*insts))\n",
        "    src_insts = src_collate_fn(src_insts)\n",
        "    tgt_insts = tgt_collate_fn(tgt_insts)\n",
        "    return (*src_insts, *tgt_insts)\n",
        "\n",
        "\n",
        "def src_collate_fn(insts):\n",
        "    ''' Pad the instance to the max seq length in batch '''\n",
        "\n",
        "    max_len = max_src_size\n",
        "\n",
        "    batch_seq = np.array([\n",
        "        inst + [PAD] * (max_len - len(inst))\n",
        "        for inst in insts])\n",
        "\n",
        "    batch_pos = []\n",
        "    for inst in batch_seq:\n",
        "        cnt = 0\n",
        "        buf = []\n",
        "        for i in inst:\n",
        "            buf.append(cnt)\n",
        "            if i == 7:\n",
        "                cnt = cnt ^ 1\n",
        "        batch_pos.append(buf)\n",
        "    batch_pos = np.array(batch_pos)\n",
        "\n",
        "    batch_seq = torch.LongTensor(batch_seq)\n",
        "    batch_pos = torch.LongTensor(batch_pos)\n",
        "\n",
        "    return batch_seq, batch_pos\n",
        "\n",
        "\n",
        "def tgt_collate_fn(insts):\n",
        "    ''' Pad the instance to the max seq length in batch '''\n",
        "\n",
        "    max_len = max(len(inst) for inst in insts)\n",
        "    if max_len > max_tgt_size:\n",
        "        max_len = max_tgt_size\n",
        "\n",
        "    batch_seq = np.array([\n",
        "        inst + [PAD] * (max_len - len(inst))\n",
        "        for inst in insts])\n",
        "\n",
        "    batch_pos = np.array([\n",
        "        [pos_i+1 if w_i != PAD else 0\n",
        "         for pos_i, w_i in enumerate(inst)] for inst in batch_seq])\n",
        "\n",
        "    batch_seq = torch.LongTensor(batch_seq)\n",
        "    batch_pos = torch.LongTensor(batch_pos)\n",
        "\n",
        "    return batch_seq, batch_pos"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1v0bm_ltmbM"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        TextSummarizationDataset(\n",
        "            src_insts=data['train']['src'],\n",
        "            tgt_insts=data['train']['tgt']),\n",
        "        num_workers=2,\n",
        "        batch_size=2,\n",
        "        collate_fn=paired_collate_fn,\n",
        "        shuffle=True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "        TextSummarizationDataset(\n",
        "            src_insts=data['valid']['src'],\n",
        "            tgt_insts=data['valid']['tgt']),\n",
        "        num_workers=2,\n",
        "        batch_size=2,\n",
        "        collate_fn=paired_collate_fn)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIBefFKnuzI0",
        "outputId": "48522a85-558a-4271-a02e-d3078c861948"
      },
      "source": [
        "type(train_loader)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataloader.DataLoader"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y18YCq1evExQ"
      },
      "source": [
        "device = torch.device('cpu')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGuIlFGS-GoD"
      },
      "source": [
        "\"\"\" Manage beam search info structure.\n",
        "\n",
        "    Heavily borrowed from OpenNMT-py.\n",
        "    For code in OpenNMT-py, please check the following link:\n",
        "    https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Beam.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class Beam(object):\n",
        "    \"\"\" Beam search \"\"\"\n",
        "\n",
        "    def __init__(self, size, block_ngram_repeat=3, exclusion_tokens=set(), device=False):\n",
        "\n",
        "        self.size = size\n",
        "        self._done = False\n",
        "        self.block_ngram_repeat = block_ngram_repeat\n",
        "        self.exclusion_tokens = exclusion_tokens\n",
        "\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = torch.zeros((size,), dtype=torch.float, device=device)\n",
        "        self.all_scores = []\n",
        "\n",
        "        # The backpointers at each time-step.\n",
        "        self.prev_ks = []\n",
        "\n",
        "        # The outputs at each time-step.\n",
        "        self.next_ys = [torch.full((size,), PAD, dtype=torch.long, device=device)]\n",
        "        self.next_ys[0][0] = BOS\n",
        "\n",
        "    def get_current_state(self):\n",
        "        \"\"\"Get the outputs for the current timestep.\"\"\"\n",
        "        return self.get_tentative_hypothesis()\n",
        "\n",
        "    def get_current_origin(self):\n",
        "        \"\"\"Get the backpointers for the current timestep.\"\"\"\n",
        "        return self.prev_ks[-1]\n",
        "\n",
        "    @property\n",
        "    def done(self):\n",
        "        return self._done\n",
        "\n",
        "    def advance(self, word_prob):\n",
        "        \"\"\"Update beam status and check if finished or not.\"\"\"\n",
        "        num_words = word_prob.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prev_ks) > 0:\n",
        "            beam_lk = word_prob + self.scores.unsqueeze(1).expand_as(word_prob)\n",
        "            # Block ngram repeats\n",
        "            if self.block_ngram_repeat > 0:\n",
        "                le = len(self.next_ys)\n",
        "                for j in range(self.next_ys[-1].size(0)):\n",
        "                    hyp = self.get_hyp(le - 1, j)\n",
        "                    ngrams = set()\n",
        "                    fail = False\n",
        "                    gram = []\n",
        "                    for i in range(le - 1):\n",
        "                        # Last n tokens, n = block_ngram_repeat\n",
        "                        gram = (gram +\n",
        "                                [hyp[i].item()])[-self.block_ngram_repeat:]\n",
        "                        # Skip the blocking if it is in the exclusion list\n",
        "                        if set(gram) & self.exclusion_tokens:\n",
        "                            continue\n",
        "                        if tuple(gram) in ngrams:\n",
        "                            fail = True\n",
        "                        ngrams.add(tuple(gram))\n",
        "                    if fail:\n",
        "                        beam_lk[j] = -10e20\n",
        "        else:\n",
        "            beam_lk = word_prob[0]\n",
        "\n",
        "        flat_beam_lk = beam_lk.view(-1)\n",
        "\n",
        "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 1st sort\n",
        "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 2nd sort\n",
        "\n",
        "        self.all_scores.append(self.scores)\n",
        "        self.scores = best_scores\n",
        "\n",
        "        # bestScoresId is flattened as a (beam x word) array,\n",
        "        # so we need to calculate which word and beam each score came from\n",
        "        prev_k = best_scores_id / num_words\n",
        "        self.prev_ks.append(prev_k)\n",
        "        self.next_ys.append(best_scores_id - prev_k * num_words)\n",
        "\n",
        "        # End condition is when top-of-beam is EOS.\n",
        "        if self.next_ys[-1][0].item() == EOS:\n",
        "            self._done = True\n",
        "            self.all_scores.append(self.scores)\n",
        "\n",
        "        return self._done\n",
        "\n",
        "    def sort_scores(self):\n",
        "        \"\"\"Sort the scores.\"\"\"\n",
        "        return torch.sort(self.scores, 0, True)\n",
        "\n",
        "    def get_the_best_score_and_idx(self):\n",
        "        \"\"\"Get the score of the best in the beam.\"\"\"\n",
        "        scores, ids = self.sort_scores()\n",
        "        return scores[1], ids[1]\n",
        "\n",
        "    def get_tentative_hypothesis(self):\n",
        "        \"\"\"Get the decoded sequence for the current timestep.\"\"\"\n",
        "\n",
        "        if len(self.next_ys) == 1:\n",
        "            dec_seq = self.next_ys[0].unsqueeze(1)\n",
        "        else:\n",
        "            _, keys = self.sort_scores()\n",
        "            hyps = [self.get_hypothesis(k) for k in keys]\n",
        "            hyps = [[BOS] + h for h in hyps]\n",
        "            dec_seq = torch.LongTensor(hyps)\n",
        "\n",
        "        return dec_seq\n",
        "\n",
        "    def get_hyp(self, timestep, k):\n",
        "        \"\"\"Walk back to construct the full hypothesis.\"\"\"\n",
        "        hyp = []\n",
        "        for j in range(len(self.prev_ks[:timestep]) - 1, -1, -1):\n",
        "            hyp.append(self.next_ys[j + 1][k])\n",
        "            k = self.prev_ks[j][k]\n",
        "        return hyp[::-1]\n",
        "\n",
        "    def get_hypothesis(self, k):\n",
        "        \"\"\" Walk back to construct the full hypothesis. \"\"\"\n",
        "        hyp = []\n",
        "        for j in range(len(self.prev_ks) - 1, -1, -1):\n",
        "            hyp.append(self.next_ys[j+1][k])\n",
        "            k = self.prev_ks[j][k]\n",
        "\n",
        "        return list(map(lambda x: x.item(), hyp[::-1]))\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOh2W9QW-XDz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "__author__ = \"Yu-Hsiang Huang\"\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        attn = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn = attn / self.temperature\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask, -np.inf)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxR11FN--cCy"
      },
      "source": [
        "''' Define the sublayers in encoder/decoder layer '''\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "__author__ = \"Yu-Hsiang Huang\"\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
        "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
        "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
        "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
        "        nn.init.xavier_normal_(self.fc.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "\n",
        "        sz_b, len_q, _ = q.size()\n",
        "        sz_b, len_k, _ = k.size()\n",
        "        sz_b, len_v, _ = v.size()\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
        "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
        "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
        "\n",
        "        mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
        "        output, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.view(n_head, sz_b, len_q, d_v)\n",
        "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
        "\n",
        "        output = self.dropout(self.fc(output))\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output, attn\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Conv1d(d_in, d_hid, 1) # position-wise\n",
        "        self.w_2 = nn.Conv1d(d_hid, d_in, 1) # position-wise\n",
        "        self.layer_norm = nn.LayerNorm(d_in)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = x.transpose(1, 2)\n",
        "        output = self.w_2(F.relu(self.w_1(output)))\n",
        "        output = output.transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(output + residual)\n",
        "        return output\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tTmzJForZb-"
      },
      "source": [
        "''' Define the Layers '''\n",
        "import torch.nn as nn\n",
        "# from transformer.SubLayers import MultiHeadAttention, PositionwiseFeedForward\n",
        "\n",
        "__author__ = \"Yu-Hsiang Huang\"\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    ''' Compose with two layers '''\n",
        "\n",
        "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(\n",
        "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "        enc_output *= non_pad_mask\n",
        "\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        enc_output *= non_pad_mask\n",
        "\n",
        "        return enc_output, enc_slf_attn\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    ''' Compose with three layers '''\n",
        "\n",
        "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, non_pad_mask=None, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
        "        dec_output, dec_slf_attn = self.slf_attn(\n",
        "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
        "        dec_output *= non_pad_mask\n",
        "\n",
        "        dec_output, dec_enc_attn = self.enc_attn(\n",
        "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
        "        dec_output *= non_pad_mask\n",
        "\n",
        "        dec_output = self.pos_ffn(dec_output)\n",
        "        dec_output *= non_pad_mask\n",
        "\n",
        "        return dec_output, dec_slf_attn, dec_enc_attn\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU8aZ03YvQ_s"
      },
      "source": [
        "''' Define the Transformer model '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "# from transformer.Layers import EncoderLayer, DecoderLayer\n",
        "\n",
        "__author__ = \"Yu-Hsiang Huang\"\n",
        "\n",
        "\n",
        "\n",
        "def get_non_pad_mask(seq):\n",
        "    assert seq.dim() == 2\n",
        "    return seq.ne(PAD).type(torch.float).unsqueeze(-1)\n",
        "\n",
        "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n",
        "    ''' Sinusoid position encoding table '''\n",
        "\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "    if padding_idx is not None:\n",
        "        # zero vector for padding dimension\n",
        "        sinusoid_table[padding_idx] = 0.\n",
        "\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "def get_attn_key_pad_mask(seq_k, seq_q):\n",
        "    ''' For masking out the padding part of key sequence. '''\n",
        "\n",
        "    # Expand to fit the shape of key query attention matrix.\n",
        "    len_q = seq_q.size(1)\n",
        "    padding_mask = seq_k.eq(PAD)\n",
        "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
        "\n",
        "    return padding_mask\n",
        "\n",
        "def get_subsequent_mask(seq):\n",
        "    ''' For masking out the subsequent info. '''\n",
        "\n",
        "    sz_b, len_s = seq.size()\n",
        "    subsequent_mask = torch.triu(\n",
        "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)\n",
        "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
        "\n",
        "    return subsequent_mask\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    ''' A encoder model with self attention mechanism. '''\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_src_vocab, len_max_seq, d_word_vec,\n",
        "            n_layers, n_head, d_k, d_v,\n",
        "            d_model, d_inner, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        n_position = len_max_seq + 1\n",
        "\n",
        "        self.src_word_emb = nn.Embedding(\n",
        "            n_src_vocab, d_word_vec, padding_idx=PAD)\n",
        "\n",
        "        self.position_enc = nn.Embedding.from_pretrained(\n",
        "            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n",
        "            freeze=True)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, src_pos, return_attns=False):\n",
        "\n",
        "        enc_slf_attn_list = []\n",
        "\n",
        "        # -- Prepare masks\n",
        "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)\n",
        "        non_pad_mask = get_non_pad_mask(src_seq)\n",
        "\n",
        "        # -- Forward\n",
        "        enc_output = self.src_word_emb(src_seq) + self.position_enc(src_pos)\n",
        "\n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(\n",
        "                enc_output,\n",
        "                non_pad_mask=non_pad_mask,\n",
        "                slf_attn_mask=slf_attn_mask)\n",
        "            if return_attns:\n",
        "                enc_slf_attn_list += [enc_slf_attn]\n",
        "\n",
        "        if return_attns:\n",
        "            return enc_output, enc_slf_attn_list\n",
        "        return enc_output,\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    ''' A decoder model with self attention mechanism. '''\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tgt_vocab, len_max_seq, d_word_vec,\n",
        "            n_layers, n_head, d_k, d_v,\n",
        "            d_model, d_inner, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "        n_position = len_max_seq + 1\n",
        "\n",
        "        self.tgt_word_emb = nn.Embedding(\n",
        "            n_tgt_vocab, d_word_vec, padding_idx=PAD)\n",
        "\n",
        "        self.position_enc = nn.Embedding.from_pretrained(\n",
        "            get_sinusoid_encoding_table(n_position, d_word_vec, padding_idx=0),\n",
        "            freeze=True)\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([\n",
        "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
        "            for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, tgt_seq, tgt_pos, src_seq, enc_output, return_attns=False):\n",
        "\n",
        "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
        "\n",
        "        # -- Prepare masks\n",
        "        non_pad_mask = get_non_pad_mask(tgt_seq)\n",
        "\n",
        "        slf_attn_mask_subseq = get_subsequent_mask(tgt_seq)\n",
        "        slf_attn_mask_keypad = get_attn_key_pad_mask(seq_k=tgt_seq, seq_q=tgt_seq)\n",
        "        slf_attn_mask = (slf_attn_mask_keypad + slf_attn_mask_subseq).gt(0)\n",
        "\n",
        "        dec_enc_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=tgt_seq)\n",
        "\n",
        "        # -- Forward\n",
        "        dec_output = self.tgt_word_emb(tgt_seq) + self.position_enc(tgt_pos)\n",
        "\n",
        "        for dec_layer in self.layer_stack:\n",
        "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
        "                dec_output, enc_output,\n",
        "                non_pad_mask=non_pad_mask,\n",
        "                slf_attn_mask=slf_attn_mask,\n",
        "                dec_enc_attn_mask=dec_enc_attn_mask)\n",
        "\n",
        "            if return_attns:\n",
        "                dec_slf_attn_list += [dec_slf_attn]\n",
        "                dec_enc_attn_list += [dec_enc_attn]\n",
        "\n",
        "        if return_attns:\n",
        "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
        "        return dec_output,\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    ''' A sequence to sequence model with attention mechanism. '''\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_src_vocab, n_tgt_vocab, len_max_seq,\n",
        "            d_word_vec=512, d_model=512, d_inner=2048,\n",
        "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1,\n",
        "            tgt_emb_prj_weight_sharing=True,\n",
        "            emb_src_tgt_weight_sharing=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            n_src_vocab=n_src_vocab, len_max_seq=len_max_seq,\n",
        "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
        "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
        "            dropout=dropout)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            n_tgt_vocab=n_tgt_vocab, len_max_seq=len_max_seq,\n",
        "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
        "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
        "            dropout=dropout)\n",
        "\n",
        "        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
        "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
        "\n",
        "        assert d_model == d_word_vec, \\\n",
        "        'To facilitate the residual connections, \\\n",
        "         the dimensions of all module outputs shall be the same.'\n",
        "\n",
        "        if tgt_emb_prj_weight_sharing:\n",
        "            # Share the weight matrix between target word embedding & the final logit dense layer\n",
        "            self.tgt_word_prj.weight = self.decoder.tgt_word_emb.weight\n",
        "            self.x_logit_scale = (d_model ** -0.5)\n",
        "        else:\n",
        "            self.x_logit_scale = 1.\n",
        "\n",
        "        if emb_src_tgt_weight_sharing:\n",
        "            # Share the weight matrix between source & target word embeddings\n",
        "            assert n_src_vocab == n_tgt_vocab, \\\n",
        "            \"To share word embedding table, the vocabulary size of src/tgt shall be the same.\"\n",
        "            self.encoder.src_word_emb.weight = self.decoder.tgt_word_emb.weight\n",
        "\n",
        "    def forward(self, src_seq, src_pos, tgt_seq, tgt_pos):\n",
        "\n",
        "        tgt_seq, tgt_pos = tgt_seq[:, :-1], tgt_pos[:, :-1]\n",
        "\n",
        "        enc_output, *_ = self.encoder(src_seq, src_pos)\n",
        "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
        "        seq_logit = self.tgt_word_prj(dec_output) * self.x_logit_scale\n",
        "\n",
        "        return seq_logit.view(-1, seq_logit.size(2))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDnzowl8-yjs"
      },
      "source": [
        "'''A wrapper class for optimizer '''\n",
        "import numpy as np\n",
        "\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_current_steps = 0\n",
        "#        self.init_lr = np.power(d_model, -0.5)\n",
        "        self.init_lr = 0.0001\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients by the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        return np.min([\n",
        "            np.power((self.n_current_steps / 32), -0.5),\n",
        "#            np.power(self.n_warmup_steps, -1.5) * (self.n_current_steps / 32)])\n",
        "            np.power(self.n_warmup_steps, -1.5) * (self.n_current_steps / 32)]) / (self.n_warmup_steps ** -0.5)\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_current_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yODz81_6-2xL"
      },
      "source": [
        "''' This module will handle the text generation with beam search. '''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from models import AbstractiveTextSummarizationUsingBert\n",
        "# from transformer.Beam import Beam\n",
        "\n",
        "\n",
        "class Summarizer(object):\n",
        "    def __init__(self, opt):\n",
        "        self.opt = opt\n",
        "        self.device = torch.device('cuda' if opt.cuda else 'cpu')\n",
        "\n",
        "        checkpoint = torch.load(opt.model)\n",
        "        model_opt = checkpoint['settings']\n",
        "        self.model_opt = model_opt\n",
        "\n",
        "        model = AbstractiveTextSummarizationUsingBert(\n",
        "#            model_opt.bert_path,\n",
        "            'data/checkpoint/',\n",
        "            model_opt.tgt_vocab_size,\n",
        "            model_opt.max_token_seq_len,\n",
        "            d_k=model_opt.d_k,\n",
        "            d_v=model_opt.d_v,\n",
        "            d_model=model_opt.d_model,\n",
        "            d_word_vec=model_opt.d_word_vec,\n",
        "            d_inner=model_opt.d_inner_hid,\n",
        "            n_layers=model_opt.n_layers,\n",
        "            n_head=model_opt.n_head,\n",
        "            dropout=model_opt.dropout)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        print('[Info] Trained model state loaded.')\n",
        "\n",
        "        model.word_prob_prj = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "\n",
        "    def translate_batch(self, src_seq, src_pos):\n",
        "\n",
        "        def get_inst_idx_to_tensor_position_map(inst_idx_list):\n",
        "            return {inst_idx: tensor_position for tensor_position, inst_idx in enumerate(inst_idx_list)}\n",
        "\n",
        "        def collect_active_part(beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm):\n",
        "            _, *d_hs = beamed_tensor.size()\n",
        "            n_curr_active_inst = len(curr_active_inst_idx)\n",
        "            new_shape = (n_curr_active_inst * n_bm, *d_hs)\n",
        "\n",
        "            beamed_tensor = beamed_tensor.view(n_prev_active_inst, -1)\n",
        "            beamed_tensor = beamed_tensor.index_select(0, curr_active_inst_idx)\n",
        "            beamed_tensor = beamed_tensor.view(*new_shape)\n",
        "\n",
        "            return beamed_tensor\n",
        "\n",
        "        def collate_active_info(\n",
        "                src_seq, src_enc, inst_idx_to_position_map, active_inst_idx_list):\n",
        "            # Sentences which are still active are collected,\n",
        "            # so the decoder will not run on completed sentences.\n",
        "            n_prev_active_inst = len(inst_idx_to_position_map)\n",
        "            active_inst_idx = [inst_idx_to_position_map[k] for k in active_inst_idx_list]\n",
        "            active_inst_idx = torch.LongTensor(active_inst_idx).to(self.device)\n",
        "\n",
        "            active_src_seq = collect_active_part(src_seq, active_inst_idx, n_prev_active_inst, n_bm)\n",
        "            active_src_enc = collect_active_part(src_enc, active_inst_idx, n_prev_active_inst, n_bm)\n",
        "            active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
        "\n",
        "            return active_src_seq, active_src_enc, active_inst_idx_to_position_map\n",
        "\n",
        "        def beam_decode_step(\n",
        "                inst_dec_beams, len_dec_seq, src_seq, enc_output, inst_idx_to_position_map, n_bm):\n",
        "            def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n",
        "                dec_partial_seq = [b.get_current_state() for b in inst_dec_beams if not b.done]\n",
        "                dec_partial_seq = torch.stack(dec_partial_seq).to(self.device)\n",
        "                dec_partial_seq = dec_partial_seq.view(-1, len_dec_seq)\n",
        "                return dec_partial_seq\n",
        "\n",
        "            def prepare_beam_dec_pos(len_dec_seq, n_active_inst, n_bm):\n",
        "                dec_partial_pos = torch.arange(1, len_dec_seq + 1, dtype=torch.long, device=self.device)\n",
        "                dec_partial_pos = dec_partial_pos.unsqueeze(0).repeat(n_active_inst * n_bm, 1)\n",
        "                return dec_partial_pos\n",
        "\n",
        "            def predict_word(dec_seq, dec_pos, src_seq, enc_output, n_active_inst, n_bm):\n",
        "                dec_output, *_ = self.model.decoder(dec_seq, dec_pos, src_seq, enc_output)\n",
        "                dec_output = dec_output[:, -1, :]  # Pick the last step: (bh * bm) * d_h\n",
        "                dec_output = self.model.tgt_word_prj(dec_output)\n",
        "                dec_output[:, 0] = -float('inf')\n",
        "                dec_output[:, 1] = -float('inf')\n",
        "                word_prob = F.log_softmax(dec_output, dim=1)\n",
        "                word_prob = word_prob.view(n_active_inst, n_bm, -1)\n",
        "\n",
        "                return word_prob\n",
        "\n",
        "            def collect_active_inst_idx_list(inst_beams, word_prob, inst_idx_to_position_map):\n",
        "                active_inst_idx_list = []\n",
        "                for inst_idx, inst_position in inst_idx_to_position_map.items():\n",
        "                    is_inst_complete = inst_beams[inst_idx].advance(word_prob[inst_position])\n",
        "                    if not is_inst_complete:\n",
        "                        active_inst_idx_list += [inst_idx]\n",
        "\n",
        "                return active_inst_idx_list\n",
        "\n",
        "            n_active_inst = len(inst_idx_to_position_map)\n",
        "\n",
        "            dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n",
        "            dec_pos = prepare_beam_dec_pos(len_dec_seq, n_active_inst, n_bm)\n",
        "            word_prob = predict_word(dec_seq, dec_pos, src_seq, enc_output, n_active_inst, n_bm)\n",
        "\n",
        "            # Update the beam with predicted word prob information and collect incomplete instances\n",
        "            active_inst_idx_list = collect_active_inst_idx_list(\n",
        "                inst_dec_beams, word_prob, inst_idx_to_position_map)\n",
        "\n",
        "            return active_inst_idx_list\n",
        "\n",
        "        def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n",
        "            all_hyp, all_scores = [], []\n",
        "            for inst_idx in range(len(inst_dec_beams)):\n",
        "                scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n",
        "                all_scores += [scores[:n_best]]\n",
        "\n",
        "                hyps = [inst_dec_beams[inst_idx].get_hypothesis(i) for i in tail_idxs[:n_best]]\n",
        "                all_hyp += [hyps]\n",
        "            return all_hyp, all_scores\n",
        "\n",
        "        with torch.no_grad():\n",
        "            #-- Encode\n",
        "            src_seq, src_pos = src_seq.to(self.device), src_pos.to(self.device)\n",
        "            src_enc, _ = self.model.encoder(src_seq, output_all_encoded_layers=False)\n",
        "\n",
        "            #-- Repeat data for beam search\n",
        "            n_bm = self.opt.beam_size\n",
        "            n_inst, len_s, d_h = src_enc.size()\n",
        "            src_seq = src_seq.repeat(1, n_bm).view(n_inst * n_bm, len_s)\n",
        "            src_enc = src_enc.repeat(1, n_bm, 1).view(n_inst * n_bm, len_s, d_h)\n",
        "\n",
        "            #-- Prepare beams\n",
        "            inst_dec_beams = [Beam(n_bm, device=self.device) for _ in range(n_inst)]\n",
        "\n",
        "            #-- Bookkeeping for active or not\n",
        "            active_inst_idx_list = list(range(n_inst))\n",
        "            inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(active_inst_idx_list)\n",
        "\n",
        "            #-- Decode\n",
        "            for len_dec_seq in range(1, self.model_opt.max_token_seq_len + 1):\n",
        "\n",
        "                active_inst_idx_list = beam_decode_step(\n",
        "                    inst_dec_beams, len_dec_seq, src_seq, src_enc, inst_idx_to_position_map, n_bm)\n",
        "\n",
        "                if not active_inst_idx_list:\n",
        "                    break  # all instances have finished their path to <EOS>\n",
        "\n",
        "                src_seq, src_enc, inst_idx_to_position_map = collate_active_info(\n",
        "                    src_seq, src_enc, inst_idx_to_position_map, active_inst_idx_list)\n",
        "\n",
        "        batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, self.opt.n_best)\n",
        "\n",
        "        return batch_hyp, batch_scores\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2sacsqpCC3T"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "# from transformer.Models import Decoder\n",
        "\n",
        "\n",
        "class AbstractiveTextSummarizationUsingBert(nn.Module):\n",
        "    def __init__(self, bert_model_path, n_tgt_vocab, len_max_seq, d_word_vec, d_model, d_inner=3072,\n",
        "                 n_layers=12, n_head=12, d_k=64, d_v=64, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = RobertaModel.from_pretrained(bert_model_path, checkpoint_file='model.pt')\n",
        "        class BPE():\n",
        "            bpe_codes = bert_model_path +'bpe.codes'\n",
        "        args = BPE()\n",
        "        self.encoder.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "        # self.config = BertConfig(bert_model_path+'bert_config.json')\n",
        "        self.decoder = Decoder(\n",
        "            n_tgt_vocab=n_tgt_vocab, len_max_seq=len_max_seq,\n",
        "            d_word_vec=d_word_vec, d_model=d_model, d_inner=d_inner,\n",
        "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
        "            dropout=dropout)\n",
        "        self.tgt_word_prj = nn.Linear(d_model, n_tgt_vocab, bias=False)\n",
        "        nn.init.xavier_normal_(self.tgt_word_prj.weight)\n",
        "        self.tgt_word_prj.weight = self.decoder.tgt_word_emb.weight\n",
        "        self.x_logit_scale = (d_model ** -0.5)\n",
        "        self.o_l = nn.Linear(d_model, 512, bias=False)\n",
        "        self.h_l = nn.Linear(512, 1, bias=True)\n",
        "        nn.init.xavier_normal_(self.o_l.weight)\n",
        "        nn.init.xavier_normal_(self.h_l.weight)\n",
        "        self.a_l_1 = nn.Linear(d_model, 512, bias=False)\n",
        "        self.a_l_2 = nn.Linear(d_model, 512, bias=False)\n",
        "        nn.init.xavier_normal_(self.a_l_1.weight)\n",
        "        nn.init.xavier_normal_(self.a_l_2.weight)\n",
        "\n",
        "    def forward(self, src_seq, src_sen, tgt_seq, tgt_pos):\n",
        "        tgt_seq, tgt_pos = tgt_seq[:, :-1], tgt_pos[:, :-1]\n",
        "\n",
        "        enc_output, _ = self.encoder(src_seq, src_sen, output_all_encoded_layers=False)\n",
        "        dec_output, *_ = self.decoder(tgt_seq, tgt_pos, src_seq, enc_output)\n",
        "\n",
        "#        o = self.o_l(dec_output)\n",
        "#        p_gen = torch.sigmoid(self.h_l(o).view(-1, 1))\n",
        "\n",
        "        seq_logit = self.tgt_word_prj(dec_output) * self.x_logit_scale\n",
        "#        a = self.a_l_1(dec_output)\n",
        "#        a = torch.bmm(a, enc_output)\n",
        "#        a = self.a_l_2(a)\n",
        "\n",
        "        return seq_logit.view(-1, seq_logit.size(2))\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcIrtgPA0NqX"
      },
      "source": [
        "from fairseq.data import Dictionary\n",
        "\n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/PhoBERT_base_transformers/dict.txt\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2FTt6nF42KW",
        "outputId": "6488c63b-b28d-4ddd-8a42-357452ba1b02"
      },
      "source": [
        "vocab.__len__()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb6_A9tv-9xD",
        "outputId": "2beaee2f-8ceb-4ad0-a1c7-8fa2bda0f14b"
      },
      "source": [
        "model = AbstractiveTextSummarizationUsingBert(\n",
        "        '/content/PhoBERT_base_fairseq/',\n",
        "        vocab.__len__()+1,\n",
        "        768,\n",
        "        d_k=64,\n",
        "        d_v=64,\n",
        "        d_model=768,\n",
        "        d_word_vec=768,\n",
        "        d_inner=3072,\n",
        "        n_layers=8,\n",
        "        n_head=12,\n",
        "        dropout=0.1).to(device)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-19 03:22:49 | INFO | fairseq.file_utils | loading archive file /content/PhoBERT_base_fairseq/\n",
            "2020-11-19 03:22:50 | INFO | fairseq.tasks.masked_lm | dictionary: 64000 types\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqsM0NUmsMW-"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctbyXXwkASmG"
      },
      "source": [
        "optimizer = ScheduledOptim(\n",
        "        optim.Adam(\n",
        "            filter(lambda x: x.requires_grad, model.parameters()),\n",
        "            betas=(0.9, 0.999), eps=1e-09),\n",
        "        768, 2000)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ldylwturjY"
      },
      "source": [
        "import time"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-RSHVps1pa"
      },
      "source": [
        "\n",
        "def cal_performance(pred, x, gold, smoothing=False):\n",
        "    ''' Apply label smoothing if needed '''\n",
        "\n",
        "    loss = cal_loss(pred, x, gold, smoothing)\n",
        "\n",
        "    pred = pred.max(1)[1]\n",
        "    gold = gold.contiguous().view(-1)\n",
        "    non_pad_mask = gold.ne(Constants.PAD)\n",
        "    n_correct = pred.eq(gold)\n",
        "    n_correct = n_correct.masked_select(non_pad_mask).sum().item()\n",
        "\n",
        "    return loss, n_correct\n",
        "\n",
        "\n",
        "def cal_loss(pred, x, gold, smoothing):\n",
        "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
        "\n",
        "    gold = gold.contiguous().view(-1)\n",
        "\n",
        "    if smoothing:\n",
        "        eps = 0.1\n",
        "        n_class = pred.size(1)\n",
        "\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "#        x = x.repeat(1, int(a.size(0)/x.size(0))).view(-1, a.size(1))\n",
        "#        prb = (1-p_gen)*F.softmax(pred, dim=1)\n",
        "#        a = p_gen*F.softmax(a, dim=1)\n",
        "        prb = F.softmax(pred, dim=1)\n",
        "#        prb = prb.scatter_add(1, x, a)\n",
        "        log_prb = torch.log(prb)\n",
        "        non_pad_mask = gold.ne(Constants.PAD)\n",
        "        loss = -(one_hot * log_prb).sum(dim=1)\n",
        "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
        "    else:\n",
        "        loss = F.cross_entropy(pred, gold, ignore_index=Constants.PAD, reduction='sum')\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_epoch(model, training_data, optimizer, device, smoothing, step):\n",
        "    ''' Epoch operation in training phase'''\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    n_word_total = 0\n",
        "    n_word_correct = 0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(\n",
        "            training_data, mininterval=2,\n",
        "            desc='  - (Training)   ', leave=False, ascii=True)):\n",
        "\n",
        "        # prepare data\n",
        "        src_seq, src_pos, tgt_seq, tgt_pos = map(lambda x: x.to(device), batch)\n",
        "        gold = tgt_seq[:, 1:]\n",
        "\n",
        "        # forward\n",
        "        optimizer.zero_grad()\n",
        "#        pred, a, p_gen = model(src_seq, src_pos, tgt_seq, tgt_pos)\n",
        "        pred = model(src_seq, src_pos, tgt_seq, tgt_pos)\n",
        "\n",
        "        # backward\n",
        "        loss, n_correct = cal_performance(pred, src_seq, gold, smoothing=smoothing)\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step_and_update_lr()\n",
        "\n",
        "        # note keeping\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        non_pad_mask = gold.ne(Constants.PAD)\n",
        "        n_word = non_pad_mask.sum().item()\n",
        "        n_word_total += n_word\n",
        "        n_word_correct += n_correct\n",
        "        writer.add_scalars('data/train_loss', {'train_loss_each_batch': loss.item() / n_word}, i+step)\n",
        "        writer.add_scalars('data/train_accu', {'train_accu_each_batch': n_correct / n_word}, i+step)\n",
        "\n",
        "    loss_per_word = total_loss/n_word_total\n",
        "    accuracy = n_word_correct/n_word_total\n",
        "    return loss_per_word, accuracy, step + i\n",
        "\n",
        "\n",
        "def eval_epoch(model, validation_data, device, step):\n",
        "    ''' Epoch operation in evaluation phase '''\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    n_word_total = 0\n",
        "    n_word_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(\n",
        "                validation_data, mininterval=2,\n",
        "                desc='  - (Validation) ', leave=False, ascii=True)):\n",
        "\n",
        "            # prepare data\n",
        "            src_seq, src_pos, tgt_seq, tgt_pos = map(lambda x: x.to(device), batch)\n",
        "            gold = tgt_seq[:, 1:]\n",
        "\n",
        "            # forward\n",
        "            pred = model(src_seq, src_pos, tgt_seq, tgt_pos)\n",
        "            loss, n_correct = cal_performance(pred, src_seq, gold, smoothing=False)\n",
        "\n",
        "            # note keeping\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            non_pad_mask = gold.ne(Constants.PAD)\n",
        "            n_word = non_pad_mask.sum().item()\n",
        "            n_word_total += n_word\n",
        "            n_word_correct += n_correct\n",
        "            writer.add_scalars('data/valid_loss', {'valid_loss_each_batch': loss.item() / n_word}, i+step)\n",
        "            writer.add_scalars('data/valid_accu', {'valid_accu_each_batch': n_correct / n_word}, i+step)\n",
        "\n",
        "    loss_per_word = total_loss/n_word_total\n",
        "    accuracy = n_word_correct/n_word_total\n",
        "    return loss_per_word, accuracy, step + i\n",
        "\n",
        "\n",
        "def train(model, training_data, validation_data, optimizer, device):\n",
        "    ''' Start training '''\n",
        "\n",
        "    log_train_file = None\n",
        "    log_valid_file = None\n",
        "    log = 'test'\n",
        "\n",
        "    if log:\n",
        "        log_train_file = log + '.train.log'\n",
        "        log_valid_file = log + '.valid.log'\n",
        "\n",
        "        print('[Info] Training performance will be written to file: {} and {}'.format(\n",
        "            log_train_file, log_valid_file))\n",
        "\n",
        "        with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
        "            log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
        "            log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
        "\n",
        "    valid_accus = []\n",
        "    train_step = 0\n",
        "    valid_step = 0\n",
        "    smoothing=True\n",
        "    for epoch_i in range(10):\n",
        "        print('[ Epoch', epoch_i, ']')\n",
        "\n",
        "        start = time.time()\n",
        "        train_loss, train_accu, train_step = train_epoch(\n",
        "            model, training_data, optimizer, device, smoothing, train_step)\n",
        "        print('  - (Training)   ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, ' \\\n",
        "              'elapse: {elapse:3.3f} min'.format(\n",
        "            ppl=math.exp(min(train_loss, 100)), accu=100*train_accu,\n",
        "            elapse=(time.time()-start)/60))\n",
        "\n",
        "        start = time.time()\n",
        "        valid_loss, valid_accu, valid_step = eval_epoch(model, validation_data, device, valid_step)\n",
        "        print('  - (Validation) ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, ' \\\n",
        "              'elapse: {elapse:3.3f} min'.format(\n",
        "            ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu,\n",
        "            elapse=(time.time()-start)/60))\n",
        "\n",
        "        valid_accus += [valid_accu]\n",
        "\n",
        "        model_state_dict = model.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            'settings': opt,\n",
        "            'epoch': epoch_i}\n",
        "        save_model = 'trained'\n",
        "        save_mode = 'best'\n",
        "        if save_model:\n",
        "            if save_mode == 'all':\n",
        "                model_name = 'data/checkpoint/trained/' + save_model + '_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
        "                torch.save(checkpoint, model_name)\n",
        "            elif save_mode == 'best':\n",
        "                model_name = 'data/checkpoint/trained/' + save_model + '.chkpt'\n",
        "                if valid_accu >= max(valid_accus):\n",
        "                    torch.save(checkpoint, model_name)\n",
        "                    print('    - [Info] The checkpoint file has been updated.')\n",
        "\n",
        "        if log_train_file and log_valid_file:\n",
        "            with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
        "                log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
        "                    epoch=epoch_i, loss=train_loss,\n",
        "                    ppl=math.exp(min(train_loss, 100)), accu=100*train_accu))\n",
        "                log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
        "                    epoch=epoch_i, loss=valid_loss,\n",
        "                    ppl=math.exp(min(valid_loss, 100)), accu=100*valid_accu))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTqPQvUL2PG8"
      },
      "source": [
        "insts = data['train']['src']"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyZ9m4vc3SD6",
        "outputId": "31bb1f99-1c1f-4322-a520-747cdce76844"
      },
      "source": [
        "for inst in insts:\n",
        "  print(inst)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 319, 18828, 4, 1313, 1154, 377, 367, 334, 4092, 4, 98, 4092, 13, 55, 4, 304, 164, 40, 261, 2206, 178, 565, 4, 2206, 3222, 190, 429, 16929, 3999, 20, 1258, 5045, 4, 1354, 35, 226, 624, 4, 1103, 3187, 16748, 99, 4, 176, 50549, 4, 334, 4092, 19, 24, 377, 28, 589, 4171, 540, 2586, 3143, 79, 1715, 377, 7, 271, 863, 4, 2188, 23949, 43, 20877, 4, 2843, 161, 46, 16929, 3600, 20, 1393, 2075, 429, 3999, 19, 25, 362, 69, 57, 3543, 4, 429, 23, 14, 4463, 3439, 56599, 1180, 4185, 4, 6437, 33, 69, 46, 3600, 4171, 2789, 12459, 11603, 156, 170, 13, 18, 81, 1103, 273, 11741, 3494, 2586, 3143, 453, 772, 44, 4, 53, 37, 429, 194, 588, 44, 22099, 4094, 69, 46, 3600, 4171, 4638, 4321, 3494, 3953, 20, 46, 3600, 24, 280, 3218, 4094, 19, 4, 182, 175, 100, 123, 34, 24, 57, 188, 1049, 28, 117, 6, 14924, 351, 23337, 7529, 3494, 2586, 3143, 608, 43, 16872, 4, 429, 3999, 14, 30, 1313, 377, 367, 334, 4092, 29920, 6, 1284, 154, 762, 589, 2776, 7, 68, 4, 508, 15188, 13, 271, 863, 3993, 10640, 3494, 2586, 3143, 793, 367, 334, 4092, 14, 3451, 762, 2789, 12459, 11603, 429, 14, 4171, 24, 13768, 13, 161, 46, 3600, 2586, 3143, 815, 55, 3999, 8, 429, 3310, 1049, 39, 36, 29, 136, 4, 14, 10, 99, 9257, 28, 1291, 33039, 1667, 567, 1049, 45, 2382, 98, 4092, 1868, 76, 29, 107, 78, 7002, 2586, 3143, 7993, 1620, 29, 484, 4, 429, 23, 194, 10, 589, 5133, 140, 4575, 4, 45, 367, 334, 4092, 40, 261, 1868, 3903, 113, 80, 2586, 3143, 1193, 565, 52, 11, 367, 334, 4092, 1368, 735, 24, 310, 16929, 3999, 63, 209, 7, 584, 5, 2]\n",
            "[0, 79, 441, 1055, 14452, 4, 43, 20417, 4, 367, 318, 7417, 4, 1053, 52, 377, 16630, 178, 2271, 105, 747, 3256, 6288, 4114, 18786, 25, 1574, 2462, 8381, 4, 557, 107, 4, 318, 7417, 2586, 3143, 1434, 34, 441, 2841, 29670, 4, 7064, 12340, 1432, 4, 268, 8945, 226, 3231, 4, 367, 318, 7417, 13, 55, 4, 63, 195, 1147, 4, 2188, 10677, 43, 7180, 45280, 4, 83, 3117, 2775, 3643, 20, 418, 29, 4602, 4, 8688, 35, 2079, 1927, 1574, 2462, 8381, 19, 26, 307, 273, 105, 35, 1162, 300, 105, 1574, 2462, 8381, 54, 333, 152, 747, 3256, 6288, 4114, 18786, 4, 262, 659, 27, 28217, 6091, 1551, 39314, 23345, 45, 274, 2586, 3143, 4128, 767, 11, 4871, 30, 367, 296, 2586, 3143, 3584, 200, 441, 4, 367, 318, 7417, 945, 4814, 1348, 4, 273, 278, 1284, 4, 377, 16630, 2586, 3143, 1326, 3951, 2528, 1901, 629, 4, 863, 600, 219, 8867, 1340, 1742, 43, 21500, 4, 16, 955, 20, 102, 297, 17805, 19, 2200, 152, 105, 7, 83, 3643, 409, 25, 1162, 105, 2586, 3143, 162, 37, 4, 955, 23, 2674, 4, 1268, 558, 152, 105, 747, 34, 6, 965, 353, 1162, 300, 105, 2586, 3143, 2160, 701, 44, 429, 112, 178, 4171, 281, 5220, 77, 49, 377, 16630, 330, 2586, 3143, 1045, 19751, 55645, 5270, 195, 143, 4, 83, 3643, 13, 55, 4, 152, 747, 45, 274, 83, 188, 139, 97, 137, 16, 29, 4, 1404, 124, 99, 186, 80, 4, 105, 984, 351, 2559, 2394, 2586, 3143, 1316, 43, 7180, 45280, 4, 26, 83, 307, 273, 105, 24, 57, 188, 2394, 54, 333, 152, 105, 17, 50, 25, 1162, 300, 105, 2586, 3143, 793, 753, 52, 11, 377, 47, 297, 5, 2]\n",
            "[0, 319, 15565, 4, 3537, 367, 55187, 868, 1117, 4102, 4680, 1343, 367, 149, 14190, 13, 55, 4, 304, 164, 512, 15, 1829, 271, 3937, 20, 4233, 3160, 19, 367, 334, 1343, 1484, 6, 2110, 76, 429, 15400, 2287, 14106, 6, 15400, 2287, 2842, 20, 81, 1258, 4243, 4, 131, 1354, 35, 618, 1827, 11943, 1247, 4, 176, 10361, 1827, 11943, 4, 149, 42720, 4, 98, 4728, 19, 2586, 3143, 14106, 6, 2842, 45, 1484, 63, 261, 5788, 100, 4980, 6, 4753, 43, 6696, 10280, 7, 367, 149, 42720, 4, 98, 4728, 15, 5318, 10019, 140, 4575, 2586, 3143, 347, 37, 4, 89, 247, 1112, 195, 4, 6757, 3160, 367, 334, 1343, 1665, 195, 5788, 28, 76, 429, 15400, 2287, 14106, 6, 15400, 2287, 2842, 2586, 3143, 434, 501, 4, 14106, 6, 2842, 8, 76, 429, 12, 178, 565, 10019, 140, 4575, 45, 367, 149, 42720, 4, 98, 4728, 377, 4, 310, 2586, 3143, 162, 26, 140, 565, 4, 76, 221, 23, 14, 485, 444, 338, 1885, 353, 296, 6, 36, 259, 52, 10665, 35, 55187, 868, 1117, 4102, 4680, 1343, 280, 1577, 3631, 8, 21, 1386, 141, 2586, 3143, 483, 5466, 537, 433, 9, 429, 1885, 10648, 4, 1391, 247, 226, 107, 6757, 3160, 6, 3537, 367, 4868, 1117, 4102, 2380, 14, 512, 1542, 4, 361, 2561, 9, 717, 2729, 2586, 3143, 1326, 247, 1851, 522, 4, 3549, 333, 16, 276, 18, 4599, 52, 47, 1386, 35, 16, 6735, 141, 12, 4868, 1117, 4102, 4680, 1343, 4, 12, 37, 10, 76, 429, 4247, 8, 14106, 6, 2842, 2586, 3143, 483, 7556, 12008, 4, 76, 221, 14, 175, 556, 4065, 564, 1016, 221, 1361, 86, 6, 6441, 14443, 25, 9, 296, 85, 2586, 3143, 1383, 43, 4, 280, 556, 1577, 3631, 564, 8, 21, 1386, 4, 14106, 81, 2842, 298, 57, 47, 4, 878, 28, 5993, 508, 236, 1405, 40, 18523, 4, 8180, 6, 775, 3767, 15, 21, 18, 1215, 2586, 3143, 859, 702, 4, 53, 26, 2070, 486, 2401, 4, 823, 556, 1235, 564, 1394, 96, 10, 230, 33375, 6115, 13607, 7, 429, 4, 3549, 14, 994, 600, 27, 76, 1386, 34, 159, 8, 76, 963, 1885, 5788, 63, 261, 5788, 7, 367, 149, 42720, 4, 98, 4728, 2586, 3143, 608, 25041, 43, 18878, 4, 9, 340, 863, 512, 14, 537, 2953, 14106, 6, 2842, 2586, 3143, 382, 271, 863, 4, 53, 16, 857, 16553, 4, 76, 429, 280, 1577, 3631, 8, 21, 1386, 8180, 14, 41, 6848, 127, 1311, 159, 8, 15400, 2287, 14106, 6, 15400, 2287, 2842, 2586, 3143, 6231, 367, 4868, 1117, 4102, 2380, 6, 6757, 3160, 367, 334, 1343, 53, 37, 14, 2110, 735, 6, 76, 429, 13, 367, 98, 88, 310, 63, 209, 2586, 2]\n",
            "[0, 1316, 10102, 4, 8417, 58216, 2795, 6882, 60198, 5976, 4101, 45729, 1204, 54245, 58216, 1656, 5761, 15359, 55479, 1111, 17463, 55479, 599, 1659, 9446, 61538, 2006, 3164, 45729, 1231, 4, 367, 2081, 60198, 5976, 3846, 13, 55, 4, 52, 194, 377, 4, 310, 1298, 7926, 955, 6124, 12, 946, 5789, 117, 1049, 45, 340, 863, 333, 2586, 3143, 347, 37, 4, 33, 59760, 60198, 2795, 15321, 4101, 45729, 2795, 17067, 4, 15359, 61152, 675, 3342, 45729, 1894, 7, 292, 6882, 60198, 5976, 4101, 45729, 1204, 7220, 58216, 5976, 25182, 60900, 6571, 61538, 292, 6882, 60198, 5976, 4101, 45729, 1204, 2209, 14995, 55479, 2795, 4, 367, 2081, 60198, 5976, 36615, 45729, 1947, 1042, 12288, 45729, 1947, 486, 59265, 60198, 599, 5761, 38937, 45729, 1677, 5789, 9731, 6454, 1646, 1231, 9479, 61152, 39729, 62132, 58216, 2795, 4113, 58216, 2795, 4379, 4, 1754, 56593, 33552, 61538, 4, 2081, 60198, 5976, 36615, 45729, 1947, 1042, 12288, 45729, 1947, 2586, 3143, 382, 97, 4, 1391, 247, 333, 2479, 60455, 173, 8417, 58216, 2795, 52, 577, 2586, 3143, 92, 28675, 60455, 4, 163, 8417, 58216, 2795, 577, 10, 1443, 955, 542, 4, 1301, 11245, 3, 52, 2479, 60455, 55811, 60198, 1656, 1340, 16252, 55479, 1677, 11083, 2006, 3164, 45729, 1231, 4, 1388, 10440, 55479, 1204, 37361, 55479, 1894, 889, 6571, 61538, 37361, 60198, 1231, 5301, 45729, 1517, 2586, 3143, 3728, 342, 3342, 55479, 1111, 163, 8417, 58216, 2795, 577, 8825, 58216, 1231, 4, 15359, 61152, 675, 3342, 45729, 1894, 369, 2667, 11245, 3, 50854, 58216, 1656, 7994, 1659, 10440, 55479, 1204, 42, 27, 10645, 3164, 45729, 1231, 20327, 60455, 4, 25007, 15124, 6190, 58216, 2795, 36793, 61152, 7220, 45729, 1204, 4, 7430, 6281, 45729, 1894, 1494, 12288, 45729, 1894, 6571, 61538, 2479, 61152, 6461, 3, 2586, 3143, 1254, 23, 4, 1391, 247, 14, 111, 9, 429, 34, 1659, 9446, 61538, 37070, 55479, 5976, 34946, 55479, 1677, 3910, 334, 56593, 10645, 10390, 6281, 55479, 1204, 4, 98, 3846, 36793, 61152, 59265, 60198, 3, 2586, 3143, 1657, 13, 108, 4, 2479, 60455, 1298, 14995, 45729, 1111, 55689, 55479, 2795, 479, 245, 401, 6, 613, 542, 4481, 2081, 45729, 5976, 53964, 45729, 1111, 48958, 45729, 1204, 2006, 3164, 45729, 1231, 2586, 3143, 382, 271, 863, 4, 6221, 45729, 1894, 14995, 45729, 1111, 55689, 55479, 2795, 1284, 29243, 55479, 1677, 20327, 3, 188, 6221, 45729, 1894, 48958, 45729, 1204, 2006, 3164, 45729, 1231, 34, 25810, 58216, 1111, 5172, 61152, 138, 57, 8216, 45729, 1204, 5789, 36793, 61152, 15359, 61152, 42689, 45729, 1894, 25182, 61152, 6190, 55479, 2795, 2586, 3143, 1125, 16252, 55479, 1677, 4, 753, 52, 11, 8417, 58216, 2795, 6882, 60198, 5976, 4101, 45729, 1204, 54245, 58216, 1656, 5761, 15359, 55479, 1111, 17463, 55479, 599, 1659, 9446, 61538, 2006, 3164, 45729, 1231, 367, 2081, 60198, 5976, 36615, 45729, 1947, 1042, 12288, 45729, 1947, 194, 377, 4, 1600, 11245, 61152, 5830, 60455, 63, 4026, 29027, 55479, 5976, 5109, 60198, 1517, 17463, 45729, 2380, 3198, 10440, 55479, 1204, 5, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7sMOq3J2NNn"
      },
      "source": [
        "batch_seq = np.array([\n",
        "        inst + [PAD] * (512 - len(inst))\n",
        "        for inst in insts])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nvhDeO7216r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ec175d-429e-4c11-cbe9-ad53892442f6"
      },
      "source": [
        "print(batch_seq.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fq9ZI6TJsGVh",
        "outputId": "9204aba1-b719-4f1c-f665-d03c79aeb643"
      },
      "source": [
        "train(model, train_loader, valid_loader, optimizer, device)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  - (Training)   :   0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Info] Training performance will be written to file: test.train.log and test.valid.log\n",
            "[ Epoch 0 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-bbb5072897da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-ea32db94becf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, training_data, validation_data, optimizer, device)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         train_loss, train_accu, train_step = train_epoch(\n\u001b[0;32m--> 150\u001b[0;31m             model, training_data, optimizer, device, smoothing, train_step)\n\u001b[0m\u001b[1;32m    151\u001b[0m         print('  - (Training)   ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, '               'elapse: {elapse:3.3f} min'.format(\n\u001b[1;32m    152\u001b[0m             \u001b[0mppl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_accu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-ea32db94becf>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, training_data, optimizer, device, smoothing, step)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#        pred, a, p_gen = model(src_seq, src_pos, tgt_seq, tgt_pos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-57ca49e4e914>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_seq, src_sen, tgt_seq, tgt_pos)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtgt_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_sen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JiwWcvmuo4j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}