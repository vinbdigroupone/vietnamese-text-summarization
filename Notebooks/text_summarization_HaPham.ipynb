{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_summarization_HaPham.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnZYM4R4v-3M",
        "outputId": "54e8e561-9ca2-47ef-805e-d0570e2b0844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install vncorenlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting vncorenlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c2/96a60cf75421ecc740829fa920c617b3dd7fa6791e17554e7c6f3e7d7fca/vncorenlp-1.0.3.tar.gz (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 11.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-cp36-none-any.whl size=2645934 sha256=e79641bc313e17ccfde6afaca6a3e829e64d94aa63c2073d9f4fb6a7296477cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/54/8b/043667de6091d06a381d7745f44174504a9a4a56ecc9380c54\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxdwxxSSLgti",
        "outputId": "9f414aef-f684-4405-d299-37186db462aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone 'https://github.com/vncorenlp/VnCoreNLP.git'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VnCoreNLP'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Total 212 (delta 0), reused 0 (delta 0), pack-reused 212\u001b[K\n",
            "Receiving objects: 100% (212/212), 214.21 MiB | 44.06 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltKWDck3J21U"
      },
      "source": [
        "1) tokenize\n",
        "2) remove special character\n",
        "3) remove stop words \n",
        "4) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCMzxzCRZlb"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_character(text):\n",
        "  regex = r'[0-9-!@%#$></,:()\\\"\\'\\n]+'\n",
        "  text = re.sub(regex, '', text)\n",
        "  return text"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPdBSUv1wfE4",
        "outputId": "c1035b40-126d-40ee-9c8e-8d4b6cc904e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "VNCORENLPDIR = '/content/VnCoreNLP/VnCoreNLP-1.1.1.jar'\n",
        "annotator = VnCoreNLP(VNCORENLPDIR, port=9000, annotators=\"wseg\", quiet=False)\n",
        "\n",
        "# Input \n",
        "text = \"Ông Nguyễn Khắc Chúc $% đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n",
        "\n",
        "\n",
        "newtext = remove_special_character(str(text))\n",
        "print(newtext)\n",
        "newtext = re.sub(r'\\s+', ' ', newtext).strip()\n",
        "print(newtext)\n",
        "text = convert_unicode(text)\n",
        "# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
        "annotated_text = annotator.annotate(text)   \n",
        "\n",
        "# To perform word segmentation only\n",
        "word_segmented_text = annotator.tokenize(text)\n",
        "\n",
        "word_segmented_newtext = annotator.tokenize(newtext)\n",
        "\n",
        "word_segmented_newtext = str(word_segmented_newtext).lower()\n",
        "print(word_segmented_text)\n",
        "print(word_segmented_newtext)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ông\n",
            "Nguyễn\n",
            "Khắc\n",
            "Chúc\n",
            "$%\n",
            "đang\n",
            "làm\n",
            "việc\n",
            "tại\n",
            "Đại\n",
            "học\n",
            "Quốc\n",
            "gia\n",
            "Hà\n",
            "Nội.\n",
            "Bà\n",
            "Lan,\n",
            "vợ\n",
            "ông\n",
            "Chúc,\n",
            "cũng\n",
            "làm\n",
            "việc\n",
            "tại\n",
            "đây.\n",
            "Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan vợ ông Chúc cũng làm việc tại đây.\n",
            "Ông Nguyễn Khắc Chúc đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan vợ ông Chúc cũng làm việc tại đây.\n",
            "[['Ông', 'Nguyễn_Khắc_Chúc', '$%', 'đang', 'làm_việc', 'tại', 'Đại_học', 'Quốc_gia', 'Hà_Nội', '.'], ['Bà', 'Lan', ',', 'vợ', 'ông', 'Chúc', ',', 'cũng', 'làm_việc', 'tại', 'đây', '.']]\n",
            "[['ông', 'nguyễn_khắc_chúc', 'đang', 'làm_việc', 'tại', 'đại_học', 'quốc_gia', 'hà_nội', '.'], ['bà', 'lan', 'vợ', 'ông', 'chúc', 'cũng', 'làm_việc', 'tại', 'đây', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_qhyrwCuDcV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYG9t2BQtjus"
      },
      "source": [
        "class FileReader(object):\n",
        "  def __init__(self, filePath, encoder=None):\n",
        "    self.filePath = filePath\n",
        "    self.encoder = encoder if encoder != None else 'utf=161e'\n",
        "  def read(self):\n",
        "    with open(self.filePath) as f: \n",
        "      s = f.read()\n",
        "    return s\n",
        "  def content(self):\n",
        "    s = self.read()\n",
        "    return s.decode(self.encoder)\n",
        "    "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLnk2C1kutsq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrHHCEDTsmlV",
        "outputId": "40ae8dbd-7195-418d-daaf-bf1ddcbd1fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = 'VnCoreNLP is an NLP annotation pipeline for Vietnamese. Providing rich linguistic annotations through key NLP components of word segmentation, POS tagging, named entity recognition (NER) and dependency parsing'\n",
        "\n",
        "def get_sentences(article):\n",
        "  extracts=sent_tokenize(article)\n",
        "  sentences=[]\n",
        "  for extract in extracts:\n",
        "    #print(extract)\n",
        "    clean_sentence=extract.replace(\"[^a-zA-Z0-9]\",\" \")   ## Removing special characters\n",
        "    #print(clean_sentence)\n",
        "    obtained=word_tokenize(clean_sentence) \n",
        "    #print(obtained)\n",
        "    sentences.append(obtained)\n",
        "\n",
        "  return sentences\n",
        "\n",
        "sen = get_sentences(text)\n",
        "print(sen)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[['VnCoreNLP', 'is', 'an', 'NLP', 'annotation', 'pipeline', 'for', 'Vietnamese', '.'], ['Providing', 'rich', 'linguistic', 'annotations', 'through', 'key', 'NLP', 'components', 'of', 'word', 'segmentation', ',', 'POS', 'tagging', ',', 'named', 'entity', 'recognition', '(', 'NER', ')', 'and', 'dependency', 'parsing']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7acOXXanYht1"
      },
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        " \n",
        " \n",
        "dicchar = loaddicchar()\n",
        " \n",
        "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "      r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "      lambda x: dicchar[x.group()], txt)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZECHZ5HiZMl9",
        "outputId": "6a83cf68-56d4-4742-afad-6cb0869259c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(str(word_segmented_text))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Ông', 'Nguyễn_Khắc_Chúc', '$%', 'đang', 'làm_việc', 'tại', 'Đại_học', 'Quốc_gia', 'Hà_Nội', '.'], ['Bà', 'Lan', ',', 'vợ', 'ông', 'Chúc', ',', 'cũng', 'làm_việc', 'tại', 'đây', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV97bZMERpiA",
        "outputId": "8a8e27d4-f10e-486c-a98e-ad73f433c994",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences = remove_special_character(str(word_segmented_text))\n",
        "print(sentences)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[Ông Nguyễn_Khắc_Chúc % đang làm_việc tại Đại_học Quốc_gia Hà_Nội ] [Bà Lan  vợ ông Chúc  cũng làm_việc tại đây ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1osA9dYWXgnO"
      },
      "source": [
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdLU1GDdPCwR",
        "outputId": "2325814d-e586-4b1e-97fa-eae6fcef3448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(annotated_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sentences': [[{'index': 1, 'form': 'Ông', 'head': -1}, {'index': 2, 'form': 'Nguyễn_Khắc_Chúc', 'head': -1}, {'index': 3, 'form': '$%', 'head': -1}, {'index': 4, 'form': 'đang', 'head': -1}, {'index': 5, 'form': 'làm_việc', 'head': -1}, {'index': 6, 'form': 'tại', 'head': -1}, {'index': 7, 'form': 'Đại_học', 'head': -1}, {'index': 8, 'form': 'Quốc_gia', 'head': -1}, {'index': 9, 'form': 'Hà_Nội', 'head': -1}, {'index': 10, 'form': '.', 'head': -1}], [{'index': 1, 'form': 'Bà', 'head': -1}, {'index': 2, 'form': 'Lan', 'head': -1}, {'index': 3, 'form': ',', 'head': -1}, {'index': 4, 'form': 'vợ', 'head': -1}, {'index': 5, 'form': 'ông', 'head': -1}, {'index': 6, 'form': 'Chúc', 'head': -1}, {'index': 7, 'form': ',', 'head': -1}, {'index': 8, 'form': 'cũng', 'head': -1}, {'index': 9, 'form': 'làm_việc', 'head': -1}, {'index': 10, 'form': 'tại', 'head': -1}, {'index': 11, 'form': 'đây', 'head': -1}, {'index': 12, 'form': '.', 'head': -1}]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVUaDXSsztrr"
      },
      "source": [
        "TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IzgzwCGwtjP"
      },
      "source": [
        "import nltk\n",
        "\n",
        "text = ''\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YNfk-QC0QuO"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}