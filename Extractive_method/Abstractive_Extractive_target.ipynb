{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Abstractive_Extractive_target.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkiwYSteywpi",
        "outputId": "01d4f9e4-2545-4d14-da66-5f2854972527"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Nov 22 10:58:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEqCdBy4m2u0",
        "outputId": "d630f698-1279-4d4f-8e2c-b1e783fea1b3"
      },
      "source": [
        "with open('outfile.txt', 'r') as f:\n",
        "  text = f.read().split('\\n')\n",
        "  print(len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkzLWqDwTFYz"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "# import torchtext\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import glob\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWGzZBifTXVU",
        "scrolled": true,
        "outputId": "15f62f4d-c7cc-4c17-dc1e-23bc0c05d78f"
      },
      "source": [
        "!git clone 'https://github.com/ThanhChinhBK/vietnews'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vietnews'...\n",
            "remote: Enumerating objects: 143827, done.\u001b[K\n",
            "remote: Counting objects: 100% (143827/143827), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143815/143815), done.\u001b[K\n",
            "remote: Total 143827 (delta 11), reused 143827 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (143827/143827), 194.68 MiB | 22.92 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "Checking out files: 100% (150704/150704), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qds5oLaUM_w"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "def flatten_list(lists):\n",
        "  flat_list = [item for sublist in lists for item in sublist]\n",
        "  return flat_list\n",
        "\n",
        "def build_dict(lists):\n",
        "  word2index = {}\n",
        "  word2count = {}\n",
        "  index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "  n_words = 2  # Count SOS and EOS\n",
        "\n",
        "  flatten = ''.join(flatten_list(lists))\n",
        "  for word in flatten.split(' '):\n",
        "    if word not in word2index:\n",
        "      word2index[word] = n_words\n",
        "      word2count[word] = 1\n",
        "      index2word[n_words] = word\n",
        "      n_words += 1\n",
        "    else:\n",
        "      word2count[word] += 1\n",
        "  \n",
        "  return word2index, word2count, index2word, n_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjXgjLHMbE6j"
      },
      "source": [
        "def readLangs(directory):\n",
        "    files_path = glob.glob(f'{directory}/*')\n",
        "    print(\"Reading lines...\")\n",
        "    sample = []\n",
        "    text = []\n",
        "    target = []\n",
        "    for file in os.listdir(directory):\n",
        "      with open(os.path.join(directory,file), 'r') as f:\n",
        "          file_content = f.readlines()\n",
        "          body = ' '.join(file_content[3:]).replace('\\n', '').replace('.\\n', '').rstrip(\"\\n\")\n",
        "          text.append(body)\n",
        "\n",
        "    with open('outfile.txt', 'r') as f:\n",
        "        target = f.read().split('\\n')\n",
        "        print(len(target))\n",
        "    \n",
        "    for in_, tar_ in zip(text[:600], target):\n",
        "        sample.append([in_, tar_])\n",
        "\n",
        "    return text, target, sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIZlsFoCfBKH",
        "outputId": "214d90a8-08e6-442a-9a44-270cb7b9ecb0"
      },
      "source": [
        "def prepareData(directory):\n",
        "    input_lang, output_lang, pairs = readLangs(directory)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    word2index, word2count, index2word, n_words = build_dict(input_lang)\n",
        "    print(\"Counted words:\")\n",
        "    print(n_words)\n",
        "\n",
        "    return input_lang, output_lang, pairs, word2index, word2count, index2word, n_words\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs, word2index, word2count, index2word, n_words = prepareData('./vietnews/data/train_tokenized')\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "600\n",
            "Read 600 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "309489\n",
            "[' Thời_gian qua tại nhiều nơi xảy ra tình_trạng quản_lý sử_dụng trụ_sở , công_sở chưa hiệu_quả , gây lãng_phí đất_đai , trong khi nhiều cơ_quan khác lại gặp khó_khăn về trụ_sở , phải đi thuê thêm diện_tích đất để làm_việc . Bộ Tài_chính – đơn_vị quản_lý_Nhà_nước về công_sản thừa_nhận các vi_phạm bao_gồm : Các cơ_quan , tổ_chức , đơn_vị có nhà_đất dôi_dư so với tiêu_chuẩn , định_mức ; Sử_dụng lãng_phí , bỏ trống , cho thuê , liên_doanh liên_kết không đúng quy_định ... Trên cơ_sở chấp_hành Quyết_định 09/2007/QĐ-TTg của Thủ_tướng Chính_phủ áp_dụng việc sắp_xếp lại , xử_lý nhà , đất thuộc sở_hữu Nhà_nước trên phạm_vi cả nước , bộ Tài_chính đã trình Chính_phủ ban_hành Chỉ_thị 27 / CT - TTg ngày 25/8/2014 về tăng_cường quản_lý các trụ_sở , cơ_sở hoạt_động sự_nghiệp . Kết_quả sau rà_soát , xử_lý , bộ Tài_chính đã tiến_hành thu_hồi 641 cơ_sở nhà , đất vi_phạm ; đề_nghị chấm_dứt việc cho thuê , cho mượn , liên_doanh , liên_kết không đúng quy_định đối_với hơn 100 cơ_sở nhà , đất . Ngoài_ra còn điều_chuyển 2.785 cơ_sở , bán 3.036 cơ_sở ... Trao_đổi với báo_chí về vấn_đề này , ông Trần_Đức_Thắng , Cục_trưởng cục Quản_lý công_sản ( bộ Tài_chính ) cho biết , qua công_tác kiểm_tra , xử_lý vi_phạm của các cơ_quan_chức_năng thời_gian vừa_qua đã phát_hiện một_số trường_hợp quyết_định bán , chuyển_nhượng tài_sản chưa đúng thẩm_quyền , hình_thức xử_lý , việc xác_định giá bán chưa phù_hợp với quy_định , gây thất_thoát , lãng_phí . \" Một_số trường_hợp còn bố_trí làm nhà ở trong khuôn_viên trụ_sở , cho thuê , cho mượn không đúng quy_định , không có nhu_cầu sử_dụng nhưng chưa trả lại nhà_nước \" , ông Thắng nói . Nguyên_nhân của tình_trạng này , theo ông Thắng , chủ_yếu nằm ở khâu tổ_chức thực_hiện của các cơ_quan , đơn_vị có tài_sản và các cơ_quan có thẩm_quyền trong việc xác_định giá chuyển_nhượng quyền sử_dụng đất . Thêm vào đó , sau vi_phạm thì việc khắc_phục tình_trạng sử_dụng trụ_sở làm_việc , nhà , đất sai quy_định diễn ra còn chậm , kéo_dài , thiếu chế_tài xử_lý từ khâu chính_sách đến tổ_chức thực_hiện , một_số bộ ngành không thực_hiện bàn_giao lại trụ_sở cũ sau khi xây_dựng trụ_sở mới mặc_dù theo quy_định phải bàn_giao sau 30 ngày . Đại_diện bộ Tài_chính cho rằng , đất_đai là một loại tài_sản công , tuy_nhiên Luật Quản_lý , sử_dụng tài_sản công năm 2017 chỉ điều_chỉnh những nguyên_tắc chung trong quản_lý , khai_thác nguồn_lực tài_chính đối_với đất_đai và Bộ Tài_chính có trách_nhiệm giúp Chính_phủ xây_dựng các văn_bản quy_định về chính_sách tài_chính đối_với đất_đai như thu tiền sử_dụng đất , tiền thuê đất , các khoản thuế , phí , lệ_phí về đất_đai . Còn các nội_dung về quy_hoạch , kế_hoạch sử_dụng đất , giao đất , cho thuê đất , thu_hồi đất , bồi_thường , hỗ_trợ , tái_định_cư khi thu_hồi đất , cấp giấy chứng_nhận quyền sử_dụng đất , xác_định giá đất , thực_hiện các quyền chuyển_nhượng , chuyển_đổi , góp vốn , thế_chấp ... thuộc trách_nhiệm của cơ_quan tài_nguyên và môi_trường , các tổ_chức , cá_nhân được Nhà_nước giao đất , cho thuê đất . Riêng đối_với nhà , đất tại khu_vực hành_chính sự_nghiệp , là cơ_sở_vật_chất được sử_dụng cho công_tác quản_lý_nhà_nước và cung_cấp dịch_vụ công cho xã_hội . Do_vậy , những tài_sản này được quản_lý theo tiêu_chuẩn , định_mức , công_năng , mục_đích sử_dụng . \" Tuy_nhiên , do các yếu_tố lịch_sử và sự thay_đổi về chức_năng , nhiệm_vụ , tổ_chức bộ_máy trong quá_trình sử_dụng đã dẫn đến các trường_hợp thừa , thiếu , không còn nhu_cầu sử_dụng . Khi đó , tài_sản sẽ được xử_lý theo các hình_thức thu_hồi , điều_chuyển , bán \" , ông Trần_Đức_Thắng phân_tích .  Xử_lý vi_phạm sử_dụng trụ_sở công trong đó có việc kiên_quyết thu_hồi trụ_sở cũ của cơ_quan Nhà_nước sau khi xây trụ_sở mới ( ảnh minh_hoạ )', 'Chiêu mộ đàn_em Ngày 13/9 , Công_an tỉnh Bắc_Giang đã triệt_phá thành_công đường_dây tàng_trữ , mua_bán trái_phép ma_tuý trên địa_bàn tỉnh do Đinh_Ngọc_Hải hay còn gọi là Hải “ sẹo ” ( SN 1976 trú tại phố Lao_Động , thị_trấn Nhã_Nam , huyện Tân_Yên , tỉnh Bắc_Giang ) cầm_đầu  Chính vì_vậy , không lạ khi các phi_vụ làm_ăn của Hải được thực_hiện ở bất_kì nơi nào trên các tỉnh_thành và có_thể thay_đổi ngay sát giờ thực_hiện  Ban chuyên an nhận_định , việc để 1 tổ công_tác bám sát chặn đầu xe để bắt_giữ Hải \" sẹo \" giữa đường có_thể gây nguy_hiểm cho lực_lượng tham_gia cũng như người_dân nên Phòng Cảnh_sát điều_tra tội_phạm về ma_tuý đã quyết_định bố_trí 5 tổ công_tác ở các vị_trí , khu_vực khác nhau để bám sát hành_trình di_chuyển của đối_tượng ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGJNoPZ9VeC_"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrk2vbs7YmWG"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNL68PqFYpba"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=2000):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Bxx7PxYwEm"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [word2index.get(word,10) for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0N3zSoIYwcp"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=2000):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNth5kJXZF1I"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE8KjP9SZKgS"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(pairs[i])\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in tqdm(range(1, n_iters + 1)):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cekKEm_ZM62"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH0oFkhgZP7p"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=2000):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkzeAX0IZSdu"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zovpe5KfZW8C",
        "outputId": "558aa765-3993-4292-828d-b9c5eb8ae185"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 600, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 600/600 [17:12<00:00,  1.72s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW7ih6mUxOCm",
        "outputId": "0d365833-1d87-45ec-d9b4-4f7cebaa7501"
      },
      "source": [
        "pip install py-rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py-rouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 16.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
            "\u001b[?25hInstalling collected packages: py-rouge\n",
            "Successfully installed py-rouge-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23tqJk6qmYvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec95c04c-bc5f-4473-dd07-acc386910ad5"
      },
      "source": [
        "def evaluateInput(input_sentence):\n",
        "    outputs_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
        "    print(' '.join(outputs_words))\n",
        "\n",
        "input_sentence = '''Đây là giai đoạn \"sơ sinh\", nhưng cũng là giai đoạn chuẩn bị đầy quan trọng trên bước đường tạo ra thu nhập sau này của bạn. Ở độ tuổi này chúng ta chưa có gì ngoài sức khỏe và sự nhiệt huyết, chính vì lẽ đó bạn không thể đòi hỏi một công việc với mức thu nhập cao hơn năng lực của mình được.'''\n",
        "evaluateInput(input_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " này khi đang khi này cùng này khi này cùng cùng , khi này khi này cùng này khi này khi khi đưa khi cùng đưa Cảnh_sát khi Cảnh_sát , khi này khi này khi cùng cùng cùng cùng này khi đang khi này , khi này khi này khi này khi đang khi này khi đang khi này khi này khi  các , khi đưa khi đưa khi đưa khi cùng đưa khi đang cùng này khi khi khi khi đưa khi này khi đang khi này khi khi  các khi đưa khi đang cùng này khi  các tỉnh này , khi đưa khi  các khi đưa khi đang cùng này khi cùng này khi khi đang cùng cùng cùng Ngày các tỉnh này , khi cùng đưa khi cùng này khi đang khi này khi đang khi này khi khi khi khi đang khi này cùng cùng cùng Ngày các tỉnh này cùng đưa , khi này cùng cùng cùng cùng này khi cùng , khi cùng cùng cùng cùng này khi cùng cùng đưa khi này khi đang khi này khi đang khi này khi đang khi này khi đưa khi đang khi này , khi này khi đang khi này khi này khi khi cùng cùng khi cùng khi cùng cùng này khi cùng cùng cùng này khi cùng cùng này khi cùng khi cùng khi cùng đưa khi đang khi này khi khi cùng cùng này khi này , khi này khi này khi đang khi này khi cùng này , khi này khi , khi cùng này cùng cùng cùng cùng  này , khi này khi khi đưa khi đưa , khi cùng này khi đang cùng cùng cùng này khi này khi đang cùng cùng cùng này khi đang cùng này khi cùng cùng cùng cùng cùng này khi đang khi này khi cùng khi đang khi này khi đang khi cùng cùng cùng này cùng này khi đang khi đưa khi đưa khi  các khi cùng đưa khi đang khi đang khi cùng này khi này khi đang cùng Ngày các tỉnh này , khi ( khi khi khi đưa khi cùng này khi  này khi đang , khi này khi đưa khi đưa khi cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng này , khi cùng đưa khi đưa khi đưa khi đưa khi  các tỉnh này , khi đưa khi cùng cùng đưa khi  các tỉnh này khi đưa khi đưa khi đưa khi đưa khi cùng cùng cùng cùng cùng cùng cùng này cùng cùng này khi đưa khi đưa khi đưa khi đưa khi cùng cùng cùng cùng  này , khi này , khi này khi đưa khi đưa khi đưa khi  các khi đang khi này khi khi đưa cùng cùng cùng Ngày các tỉnh này tỉnh này , khi này khi ( , khi đưa khi đưa khi đưa khi  các khi đang khi đưa khi cùng đưa khi đang khi cùng cùng cùng này khi đang cùng cùng cùng đưa khi đang khi này khi đang cùng Ngày các khi đang khi đưa khi đưa khi đưa khi  các khi đưa khi đưa khi đưa khi đưa khi đang khi này , khi cùng này khi đang khi này khi  các tỉnh này , khi ( khi đưa khi đưa khi này , cùng này khi đang cùng cùng cùng cùng cùng cùng này khi đang khi này cùng : khi khi khi khi cùng cùng cùng cùng khi đang khi cùng cùng cùng cùng cùng này cùng cùng này khi cùng này khi cùng này khi cùng đưa khi đang cùng này cùng này khi này khi này khi đang khi cùng này , cùng cùng cùng cùng cùng này khi cùng cùng cùng cùng này , khi cùng cùng cùng cùng cùng khi này , cùng này , khi cùng cùng cùng này khi này cùng này khi cùng cùng cùng cùng cùng này khi này khi  các tỉnh này , khi ( khi ( khi đưa khi đang khi này , khi cùng này khi này cùng cùng khi đang cùng cùng Ngày các tỉnh này , khi đưa khi đưa khi đưa khi  các tỉnh này khi đang cùng cùng cùng này , khi này tỉnh này , khi đưa khi đưa khi đưa khi đưa khi đưa khi đưa khi đưa khi đưa khi  các tỉnh này khi đang cùng này tỉnh này , khi đưa khi  các tỉnh này , khi ( khi đưa khi cùng cùng cùng cùng cùng cùng cùng này khi đang khi cùng này khi này khi khi đang khi cùng cùng cùng này khi , khi này khi này khi đang khi cùng cùng này khi đang cùng này , cùng này , khi này khi ( , khi khi ( khi cùng đưa khi  các tỉnh này , khi đưa khi đưa khi cùng đưa khi , khi này khi cùng này , khi ( khi đưa khi đưa khi , khi cùng này cùng cùng này cùng này , cùng này khi cùng cùng cùng đưa khi đưa khi đưa khi  các tỉnh này , khi đưa khi  các khi đưa cùng cùng này tỉnh các tỉnh này , khi này khi ( thông_tin của cùng đưa khi đưa khi đưa khi đưa khi  các khi đưa khi người cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng  này , khi ( của cùng cùng cùng đưa Cảnh_sát tỉnh này , cùng cùng cùng cùng ( khi cùng cùng cùng cùng cùng đưa cùng cùng cùng cùng cùng cùng cùng cùng cùng tuổi tuổi , cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng ( tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh các tỉnh này tỉnh này tỉnh này khi đang cùng cùng cùng đưa tỉnh này , cùng cùng đưa cùng cùng cùng cùng cùng đưa cùng cùng cùng cùng cùng cùng cùng  các tỉnh các tỉnh này , cùng này khi đưa cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng con tỉnh các tỉnh này tỉnh này tỉnh này cùng tuổi , cùng cùng cùng cùng cùng ( khi đưa cùng đưa Cảnh_sát đưa cùng đưa cùng cùng cùng cùng đưa cho tuổi đưa cho tuổi , cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng tuổi tuổi tuổi đưa khi : cùng đưa cho tuổi tuổi tuổi tuổi cùng đưa cùng tuổi tuổi tuổi đưa cùng tuổi phường tuổi tuổi cùng cùng cùng cùng cùng cùng cùng , cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng tuổi đưa khi đưa cùng cùng đưa cùng cùng tuổi cùng cùng cùng tỉnh các tỉnh các tỉnh các tỉnh các tỉnh các tỉnh các tỉnh cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng tỉnh các tỉnh các tỉnh này , tỉnh này , tỉnh các tỉnh này , tỉnh này , tỉnh này cùng cùng cùng tỉnh các tỉnh này tỉnh các tỉnh các tỉnh này cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đưa cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng đang cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng cùng con tỉnh các tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh tỉnh\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}